[
  {
    "objectID": "drafts/scratchpad.html",
    "href": "drafts/scratchpad.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "https://molgenis.org/ : analyse platform bioinformatics\nhttps://www.openplanet.cloud/data-platform : analyuse\nhttps://www.yivi.app/ : Identity mgnt\nhttps://checkmk.com/ : OSS operations & observability"
  },
  {
    "objectID": "drafts/scratchpad.html#lijst-van-relevante-projecten-en-componenten",
    "href": "drafts/scratchpad.html#lijst-van-relevante-projecten-en-componenten",
    "title": "Working document AI platform",
    "section": "",
    "text": "https://molgenis.org/ : analyse platform bioinformatics\nhttps://www.openplanet.cloud/data-platform : analyuse\nhttps://www.yivi.app/ : Identity mgnt\nhttps://checkmk.com/ : OSS operations & observability"
  },
  {
    "objectID": "drafts/valleys.html#context-ai4health",
    "href": "drafts/valleys.html#context-ai4health",
    "title": "Strategisch perspectief",
    "section": "Context: AI4Health",
    "text": "Context: AI4Health\nIn Nederland zijn verschillende landelijke initiatieven actief die zich richten op het verbeteren van de digitale zorg en het benutten van de grote potentie van AI daarbij:\n\nHealth-RI en Cumuluz werken aan data-beschikbaarheid\nde Nederlandse AI Coalitie (NLAIC) richt zich op AI an sich\nDigizo-Zorgverzekeraars Nederland (ZN) voor het versnellen van innovatie\n\nElk van deze initiatieven maakt op zichzelf goede voortgang, maar lopen tegen knelpunten aan die ze individueel niet kunnen oplossen. Hiervoor is overzicht, regie en nauwe samenwerking met VWS nodig en intrinsieke betrokkenheid van essentiële ketenpartijen, waaronder Nictiz, de koepels, de Patiëntenfederatie en de zorg- en onderzoeksorganisaties.\nHet is in deze context dat Health-RI, NLAIC de krachten gebundeld, met betrokkenheid van ZN, om gezamenlijk een versneld en positief AI-innovatieklimaat te creëren in zorg en onderzoek wat heeft geleid tot een eerste aanzet voor een nationaal actieplan: AI4Health.\nMet deze aanpak willen we breed barrières aangepakken die het momenteel AI-gedreven innovaties buitengewoon moeilijk maken om de opschalingsfase bereiken. Vanuit de praktijk , waardoor impact op de noodzakelijke zorgtransformatie achterblijft. De dagelijkse praktijk wijst uit dat er drie breed-gevoelde barrières (valleys of death) die overkomen moeten worden (zie Gude, van Eekeren, en Vasseur (2024) voor een recent overzicht).\n\n\n\n\n\n\nFiguur 1: De drie valleys of death die grootschalig gebruik van AI in de zorg in de weg staan.\n\n\n\n\nVan concept tot toegang tot de praktijk. Voordat grootschalig pilots en marktanalyse mogelijk is moet voldaan worden aan de voorwaarden van de Medical Device Regulation (MDR) en de AI act; 80-90% van de innovaties strandt hier. In de verschillende meetings en workshops die NLAIC georganiseerd heeft met zorginstellingen en innovatoren kwam de MDR/AI ACT bij herhaling als grote bottleneck naar boven (altijd top-3).\nVan pilot naar eerste betalende klant. Dit vereist onder meer een solide business case, betrouwbare toegang tot data en professionele ondersteuning. Dit blijft een hardnekkig probleem, niet alleen voor toepassingen in de ziekenhuiszorg maar komt ook terug bij GGZ en VVT. Binnen de ziekenhuiszorg is het adresseren van deze VoD daarom bv een speerpunt voor de SAZ-ziekenhuizen (Expertisecentrum Zorgalgoritmen).\nVan eerste klant naar duurzame opschaling. Implementatie, acceptatie en validatie (“calibratie”) in andere omgevingen dan die van de pilot sites en eerste klant zijn verre van triviaal voor AI-innovaties. De meeste innovaties die de eerste klant weten te bereiken stranden alsnog in deze Valley of Death. Ter verdere illustratie van de huidige stand van zaken: bij een AI-readiness traject van NLAIC kwam naar voren dat nog geen enkele zorginstelling daadwerkelijk AI-ready is."
  },
  {
    "objectID": "drafts/valleys.html#behoefte-aan-een-ai-dataplatform-aida",
    "href": "drafts/valleys.html#behoefte-aan-een-ai-dataplatform-aida",
    "title": "Strategisch perspectief",
    "section": "Behoefte aan een “AI dataplatform” (AIDA)",
    "text": "Behoefte aan een “AI dataplatform” (AIDA)\nOp dit moment wordt gewerkt aan een nationaal actieplan om deze barrières te adresseren langs vier actielijnen, te weten i) databeschikbaarheid, ii) AI-readiness, iii) AI-beschikbaarheid, en iv) orkestratie. Binnen de eerste actielijn databeschikbaarheid is in de afgelopen jaren het nodige in gang gezet, waaronder het systematisch toepassen van FAIR principes, een helder en breed gedragen ethisch en juridisch kader en het realiseren van gedistribueerde toegang tot data.\nTegelijkertijd kunnen we constateren dat het ontbreekt aan een generieke ‘AI dataplatform’ waarop effectief onderzoek kan worden gedaan, algoritmes kunnen worden ontwikkeld etc. Een dergelijk platform is een geïntegreerd systeem dat AI-ontwikkelaars ondersteunt met toegang tot data, modellen en andere hulpmiddelen om AI-projecten te ontwikkelen en te verbeteren. Dit platform biedt toegang tot essentiële bronnen, zoals datasets voor het trainen van AI-algoritmen, basis AI-modellen en diensten zoals een ELSA-desk voor ethische en juridische vraagstukken.\nEen dergelijk ‘AI dataplatform’ is in feite een specifieke vorm van een beveiligde verwerkingsomgeving zoals in in de datagovernance verordening artikel 2 lid 20 is gedefinieerd:\n\n“beveiligde verwerkingsomgeving”: de fysieke of virtuele omgeving en organisatorische middelen om te zorgen voor de naleving van het Unierecht, zoals Verordening (EU) 2016/679 (de Algemene verordening gegevensbescherming), met name wat betreft de rechten van datasubjecten, intellectuele-eigendomsrechten, en handels- en statistisch geheim, integriteit en toegankelijkheid, alsook van het toepasselijke nationale recht, en om de entiteit die de beveiligde verwerkingsomgeving biedt in staat te stellen alle gegevensverwerkingsactiviteiten te bepalen en er toezicht op te houden, met inbegrip van het tonen, opslaan, downloaden en exporteren van gegevens en het berekenen van afgeleide gegevens door middel van computeralgoritmen;\n\nNationaal als ook internationaal zijn er voorbeelden van dergelijke beveiligde verwerkingsomgevingen. Zo hebben de meeste National Statistics Offices (NSOs) zoals het CBS een microdata omgeving. Alhoewel deze omgevingen zijn opzet voordat machine learning zijn intrede deed, bieden de meeste microdata omgevingen nu ook al de mogelijkheid om ‘lichte’ algoritmes te trainen op tabulaire data. Een rapport van de Verenigde Naties beschrijft dat deze omgevingen in toenemende mate ook worden uitgebreid met nieuwe AI-technieken, zoals privacy-enhancing technologieën (PETs)((‘The PET Guide’ 2023)).\nSpecifiek voor de zorg zijn er ook al een aantal beveiligde verwerkingsomgevingen operationeel.\n\nHet Finse Social and Health Data Permit Authority (Findata) biedt met Kapseli een landelijke voorziening aan dat aanvullend is op het Finse NSO.\nHet Mayo Clinic Platform_Discover is een voorbeeld van een platform binnen een netwerk van zorg leveranciers.\nIn toenemende mate worden federated learning (FL) platformen gebruikt (Teo e.a. 2024). In Nederland is een actieve community rondom het vantage6 platform dat wordt gebruikt in het PLUGIN project, en internationaal in 50 andere netwerken."
  },
  {
    "objectID": "drafts/valleys.html#een-eerste-ontwerp-van-aida",
    "href": "drafts/valleys.html#een-eerste-ontwerp-van-aida",
    "title": "Strategisch perspectief",
    "section": "Een eerste ontwerp van AIDA",
    "text": "Een eerste ontwerp van AIDA\n\nIn analogie: de huidige stand van zaken zijn gefragmenteerde terpen, waarbij AI-gedreven innovaties vaak van worden ontwikkeld op niet gestandaardiseerde infrastructuur (terpen)\nIn plaats van terpen, willen we naar een deltawerken voor AI4Health.\n\nDeltaplan: het ontwerp van een gestandaardiseerd “AI dataplatform” waarmee komende jaren generieke, landelijk dekkende voorzieningen gerealiseerd kunnen worden\nDeltawerken: de realisatie van het Deltaplan, met het perspectief dat er niet één platform is, maar een ecosysteem van platformen die interoperabel zijn. Net zoals dat de Deltawerken een ecosysteem van dijken, waterkeringen is.\n\n\nDit document geeft een aanzet tot het Deltaplan voor AIDA. Het is opgebouwd in hoofdstukken die min of meer de lagen van Archimate volgen.\nWe volgen ongeveer het volgende proces:\n\njanuari/februari 2025: bureauonderzoek, eerste opzet en structuur\nfebruari/maart 2025: eerste feedback rond met architecten van de nodes\nmaart/april: publieke consultatie en rondetafel met VWS"
  },
  {
    "objectID": "drafts/mpc.html",
    "href": "drafts/mpc.html",
    "title": "Secure Multiparty Computation (MPC)",
    "section": "",
    "text": "(korte uitleg) Verschillende leveranciers in Nederland, waaronder Linksight en Roseman Labs"
  },
  {
    "objectID": "drafts/mpc.html#wat-is-mpc",
    "href": "drafts/mpc.html#wat-is-mpc",
    "title": "Secure Multiparty Computation (MPC)",
    "section": "",
    "text": "(korte uitleg) Verschillende leveranciers in Nederland, waaronder Linksight en Roseman Labs"
  },
  {
    "objectID": "drafts/mpc.html#waar-is-mpc-al-operationeel",
    "href": "drafts/mpc.html#waar-is-mpc-al-operationeel",
    "title": "Secure Multiparty Computation (MPC)",
    "section": "Waar is MPC al operationeel?",
    "text": "Waar is MPC al operationeel?\n\nTransmuraal Zorgnetwerk West-Brabant Oost\n\nhttps://www.tmz-breda.nl/\nRoseman Platform als om middels MPC een federated in-the-blind SPE aan te bieden. Platform kan tot een paar miljoen rijen tabulaire machine learning algoritmes ondersteunen\nVoorbeeld van een onderzoek: Verbeteren instroom, doorstroom en uitstroom van patiënten in de keten (link)\n\n\n\nLinksight\n\n…\n…\n…\n\n\n\nUMCU"
  },
  {
    "objectID": "business/personas/community-manager.html",
    "href": "business/personas/community-manager.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "(cl-infrastructure-community-managers)= # Community Managers: Overview\nCommunity Managers roles are well established in technical industry but only over the last years they have gained recognition within academia and scientific communities. Often these roles may not be called community managers, but their responsibilities include establishing engagement, organising community spaces and events, supporting people through inclusive practices, developing and maintaining resources, growing and evaluating use cases and collaborating with people involved in research and scientific communities.\n\n\n\n```upasexuu ../../figures/research-community-manager.*\n\n\n\n\nheight: 500px\n\n\nname: research-community-manager\n\n\nalt: Cartoon-like sketch of a person with eight arms. Two are embracing a group of people, while the others are outstretched. The person in the centre is a Research Community Manager, who has eight arms coming out of their shirt. Each arm holds a sign with text describing the main duties of a community manager. They include embedding open, inclusive and reproducible ideas, ensuring a shared understanding, facilitating stakeholder engagement and collaboration, providing technical support; co-creating, maintaining and communicating; and amplifying and championing their community.\n\n\n\nWhat does a Research Community Manager do? The Turing Way project illustration by Scriberia. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.8169292.\n\n(cl-infrastructure-community-managers-tasks)=\n## What do Community Managers do? \n\nCommunity Managers main objective is to organise groups of scientists, researchers and/or patients and the public around shared research topics and objectives. \nThey are often employed by professional societies, universities, research institutions, larger programmes, and non-profit organisations. \n\nEach role is varied but the main activities are typically focused around compassion, stewardship, and collaboration with the community: \n* Catalyse connections with all stakeholders\n* Foster communities (integrating communities together)\n* Linking the right people up together (based on expertise and interests)\n* Active community member\n* Communicating technical topics to non-technical colleagues and vice versa\n* Encouraging high standards of reproducible, ethical, inclusive and collaborative data science\n* Creating sub-networks within the community around shared experiences, for example, such as an early career researcher network\n\nThe day-to-day tasks of a community manager could include: \n* Organising and hosting community calls\n* Onboarding new members\n* Attending community leadership meetings\n* Writing community reports or newsletters\n* Maintaining and updating the community site \n* Posting on social media \n* Planning for upcoming community initiatives\n* Running training courses & workshops\n* Reading publications relevant to the community\n* Managing membership lists\n* Mentoring community contributions\n* Creating informational materials to help the wider field or public learn about the community and their projects\n* And a lot more!\n\n(cl-infrastructure-community-managers-skills)=\n## What qualifications or skills do you need to be a Community Manager? \nThe vast majority of community managers will have a scientific background that may include advanced degrees (at a masters or doctoral level). \nMany community managers also have a background related to the specific field or discipline they manage a community in, but not all of them do. \nIf the community is associated with software or programming, it is common for community managers to also have some coding skills. \n\nThere is no professionally recognised qualification or training course to become a scientific community manager, but organisations do offer training and resources to help support the professional development of people in these roles.\nThe values and approaches community managers bring to their roles are often the most important qualifications for success as a community manager.\n\n```{admonition} Community Skills and Core Competancies \nThe Community Skills Framework™ by Community Roundtable includes five skill families: Content, Technical, Business, Engagement and Strategic community management skills.\nSee the full document [online](https://communityroundtable.com/what-we-do/models-and-frameworks/community-skills-framework/) under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. \n\nThe CSCCE Skills Wheel include five core competancies: technical skills, communication, programme management, programme development and interpersonal.  \nSee the full document on [Zenodo](https://zenodo.org/record/4437294#.YYjzg07P1aQ) under the Creative Commons Attribution Non Commercial No Derivatives 4.0 International.\n(cl-infrastructure-community-managers-challenges)= ## Challenges for Community Managers * Need to mediate between community members * May not be seen as a part of the community themselves and need to build trust and credibility within it * Need to have a lot of different skills (technical, interpersonal, project management) * Supporting and encouraging engagement in the community * Building infrastructure from scratch in newly created roles * Being seen as a professional in their own right and not just adminstrative staff * Connecting with new audiences who are not aware of the community * Translating between different groups in the same field or institution * Managing tasks where there is little formal process in place * Managing the different priorities of different stakeholder groups * Not always visible when things go well! * Lack of formal career progression\n(cl-infrastructure-community-managers-benefits)= ## Benefits to having Community Managers * Able to offer meta-thinking about how the community is structured and run * Shares best practices around communication, collaboration, diversity, equity and inclusion, and other areas of research * Supports the upskilling of members via technical skill sharing and training * Supports other members of the community to take on more active roles, increasing resilience and expansion of the community * Stewarding initiatives to develop the community such as data standards, a code of conduct, or training workshops * Offers a stable base for the community, to make sure meetings happen on time * Connecting groups working on similar projects together, to support increased collaboration * Breaking down siloes between departments, fields, research groups * Greater understanding of the needs of the community\n(cl-infrastructure-community-managers-organisations)= ## Organisations that support Community Managers The Center for Scientific Collaboration and Community Engagement (cscce.org works to “professionalize and institutionalize the role of the community engagement manager (CEM) within science.” They offer training, co-created resources, research, and an active community of practice for scientific community managers to connect and support each other.\n(cl-infrastructure-communitymanagers-summary)= ## Summary Community Managers are an important part of scientific communities, supporting collaboration, best practices, and stewarding their communities as they develop. They do not have a formal career path or qualifications, but typically have a scientific or research background themselves."
  },
  {
    "objectID": "business/personas/data-wrangler.html",
    "href": "business/personas/data-wrangler.html",
    "title": "Data Wranglers: Overview",
    "section": "",
    "text": "Data Wranglers can be viewed as a specialised type of data scientist, primarily working in the space between data generators and data analysts. There are many activities that a data scientist might undertake, for example, data collection, wrangling, analysis, modelling, visualisation and communication. How these activities map onto different job titles is domain specific and will vary on a project and organisational level.\n\n\n\n\n\n\nFiguur 1: A Data Wrangler collaborates with multiple specialists to provide research-ready data whilst upholding data privacy and domain-specific standards. Created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\n\n\nIn a data science project, it is commonly observed that data wrangling tasks take the majority of time {cite:ps}anaconda2020sods, in contrast to data analysis and modelling. Traditionally, data wrangling tasks involve cleaning, restructuring and filtering data into analysis-ready formats. However, in terms of Data Wrangler as a profession, day to day tasks and objectives can be much more diverse.\nData Wranglers work primarily in the space between data generators and data analysts, who are addressing the research question of interest. Understanding the intended use for the data in the context of the analysis and research questions, Data Wranglers can be in the position of influencing data generators in improving data collection methods. Similarly, Data Wranglers will conduct preliminary analysis on the data to ensure both completeness of data and preparation for data analysis, acting as a proxy for the data generator’s knowledge during the data analysis process. A key focus of a Data Wrangler’s role is the preparation of analysis/research-ready data {cite:ps}stewart2022table in which data security, data management and FAIR standards {cite:ps}mons2017fair are all core priorities.\n\n\n\nData retrieval, querying, data analysis and visualizations\nData pipeline development, maintenance and improvement\nCommunicating with domain and technical experts\nData privacy and infrastructure setup and management\nSummarising domain specific research papers\nSharing and reviewing resources and code\nMaintaining communication across multiple teams\nRunning workshops on data wrangling and related skills\n\n\n\n\n\nData Wranglers should have experience with programming (no specific language required, but there is a wider adoption of both R and Python), database querying (SQL) and data analysis. They will have an educational background to equip them to engage with the specific research data objects relevant to the projects they will work on. Therefore, they will have undergraduate and postgraduate degrees, or equivalent experience. As with many data science and research infrastructure roles, further relevant training and specialisation can happen on the job. They need good problem solving skills, with a curiosity and willingness to learn. Lastly, good interpersonal skills are required in order to work with people with many different backgrounds, skillsets and priorities.\n\n\n\nSome key challenges of a Data Wrangler role are:\n\nImmutable data collection methods (unable to change how data has been collected and stored)\nStringent data privacy requirements\nDifficulties in accessing datasets\nLack of or missing documentation on data generation, data structure and contextual information\nInsufficient resources (human, computational, economic)\nUnclear scope of responsibilities within the project\n\nIn an ideal situation, some of these challenges can be mitigated if communication with Data Wranglers near the start of a project is encouraged and facilitated.\n\n\n\n\nImproved communication between stakeholders: Data Wranglers will mediate the ‘language barriers’ between the different people involved in collecting the data, analysing the data and interpreting the data.\nCentrally coordinated efforts: Data Wranglers can collect common questions and discussion topics of interest and provide answers and resources.\nData Wranglers will provide expertise in data collection, structuring and quality assurance. This can decrease the time it takes to go from data provider to data analysis, whilst also improving analysis quality and focus.\nData Wranglers can ensure a project keeps up to date with field specific knowledge and standards relevant to data processing and sharing, and communicate this back to the project team.\nBy participating and contributing to data analysis meetings and tasks, Data Wranglers can provide the best suitable datasets for the analysis, help understand and overcome challenges, and suggest further research paths.\n\n\n\n\nA Data Wrangler position is becoming recognised as a crucial part of any project that involves large amounts of complex data, specifically in a research context. They will have a diverse set of technical and interpersonal skills. A Data Wrangler will bring dedicated time and resources to increasing data quality whilst facilitating collaboration, ultimately resulting in more efficient and impactful project outcomes."
  },
  {
    "objectID": "business/personas/data-wrangler.html#what-do-data-wranglers-do",
    "href": "business/personas/data-wrangler.html#what-do-data-wranglers-do",
    "title": "Data Wranglers: Overview",
    "section": "",
    "text": "In a data science project, it is commonly observed that data wrangling tasks take the majority of time {cite:ps}anaconda2020sods, in contrast to data analysis and modelling. Traditionally, data wrangling tasks involve cleaning, restructuring and filtering data into analysis-ready formats. However, in terms of Data Wrangler as a profession, day to day tasks and objectives can be much more diverse.\nData Wranglers work primarily in the space between data generators and data analysts, who are addressing the research question of interest. Understanding the intended use for the data in the context of the analysis and research questions, Data Wranglers can be in the position of influencing data generators in improving data collection methods. Similarly, Data Wranglers will conduct preliminary analysis on the data to ensure both completeness of data and preparation for data analysis, acting as a proxy for the data generator’s knowledge during the data analysis process. A key focus of a Data Wrangler’s role is the preparation of analysis/research-ready data {cite:ps}stewart2022table in which data security, data management and FAIR standards {cite:ps}mons2017fair are all core priorities.\n\n\n\nData retrieval, querying, data analysis and visualizations\nData pipeline development, maintenance and improvement\nCommunicating with domain and technical experts\nData privacy and infrastructure setup and management\nSummarising domain specific research papers\nSharing and reviewing resources and code\nMaintaining communication across multiple teams\nRunning workshops on data wrangling and related skills"
  },
  {
    "objectID": "business/personas/data-wrangler.html#what-qualifications-or-skills-do-you-need-to-be-a-data-wrangler",
    "href": "business/personas/data-wrangler.html#what-qualifications-or-skills-do-you-need-to-be-a-data-wrangler",
    "title": "Data Wranglers: Overview",
    "section": "",
    "text": "Data Wranglers should have experience with programming (no specific language required, but there is a wider adoption of both R and Python), database querying (SQL) and data analysis. They will have an educational background to equip them to engage with the specific research data objects relevant to the projects they will work on. Therefore, they will have undergraduate and postgraduate degrees, or equivalent experience. As with many data science and research infrastructure roles, further relevant training and specialisation can happen on the job. They need good problem solving skills, with a curiosity and willingness to learn. Lastly, good interpersonal skills are required in order to work with people with many different backgrounds, skillsets and priorities."
  },
  {
    "objectID": "business/personas/data-wrangler.html#challenges-for-data-wranglers",
    "href": "business/personas/data-wrangler.html#challenges-for-data-wranglers",
    "title": "Data Wranglers: Overview",
    "section": "",
    "text": "Some key challenges of a Data Wrangler role are:\n\nImmutable data collection methods (unable to change how data has been collected and stored)\nStringent data privacy requirements\nDifficulties in accessing datasets\nLack of or missing documentation on data generation, data structure and contextual information\nInsufficient resources (human, computational, economic)\nUnclear scope of responsibilities within the project\n\nIn an ideal situation, some of these challenges can be mitigated if communication with Data Wranglers near the start of a project is encouraged and facilitated."
  },
  {
    "objectID": "business/personas/data-wrangler.html#benefits-of-having-data-wranglers",
    "href": "business/personas/data-wrangler.html#benefits-of-having-data-wranglers",
    "title": "Data Wranglers: Overview",
    "section": "",
    "text": "Improved communication between stakeholders: Data Wranglers will mediate the ‘language barriers’ between the different people involved in collecting the data, analysing the data and interpreting the data.\nCentrally coordinated efforts: Data Wranglers can collect common questions and discussion topics of interest and provide answers and resources.\nData Wranglers will provide expertise in data collection, structuring and quality assurance. This can decrease the time it takes to go from data provider to data analysis, whilst also improving analysis quality and focus.\nData Wranglers can ensure a project keeps up to date with field specific knowledge and standards relevant to data processing and sharing, and communicate this back to the project team.\nBy participating and contributing to data analysis meetings and tasks, Data Wranglers can provide the best suitable datasets for the analysis, help understand and overcome challenges, and suggest further research paths."
  },
  {
    "objectID": "business/personas/data-wrangler.html#data-wranglers-summary",
    "href": "business/personas/data-wrangler.html#data-wranglers-summary",
    "title": "Data Wranglers: Overview",
    "section": "",
    "text": "A Data Wrangler position is becoming recognised as a crucial part of any project that involves large amounts of complex data, specifically in a research context. They will have a diverse set of technical and interpersonal skills. A Data Wrangler will bring dedicated time and resources to increasing data quality whilst facilitating collaboration, ultimately resulting in more efficient and impactful project outcomes."
  },
  {
    "objectID": "business/personas/research-infrastructure-developer.html",
    "href": "business/personas/research-infrastructure-developer.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "(cl-infrastructure-rid)= # Research Infrastructure Developer: Overview\nResearch Infrastructure Developers (RIDs) are experts in building and maintaining the customised hardware and software infrastructure used by researchers, ranging from the very large (High Performance Computing systems and data storage services) used by thousands of users down to bespoke virtual machines for individual projects. They are highly skilled team members who can also conduct their own research as part of their role.\n(cl-infrastructure-rid-role)= ## What do Research Infrastructure Developers do?\nResearch Infrastructure Developers design and build the computational and data infrastructure for researchers, making use of best practice techniques such as software defined infrastructure, continuous integration and deployment, version control) to create systems that are performant, resiliant and easy to (re-)deploy and maintain.\nThey may be attached to a specific project or they may work as part of a team building and supporting larger, shared services (most usually both).\nBecause the role involves design responsibility for future infrastructure, RIDs may be PIs or have their own research projects related to infrastructure (for example looking at a new accelerator, network or processor technology), depending on the institution and policies of the projects they are working on.\nAs they progress in their career, RIDs may be responsible for entire large infrastructure at an institution or nationally."
  },
  {
    "objectID": "business/personas/research-infrastructure-developer.html#who-are-research-infrastructure-developers",
    "href": "business/personas/research-infrastructure-developer.html#who-are-research-infrastructure-developers",
    "title": "Working document AI platform",
    "section": "Who are Research Infrastructure Developers?",
    "text": "Who are Research Infrastructure Developers?\nResearch Infrastructure Developers come from a wide variety of backgrounds - some are professional system administrators or experienced DevOps professionals who have joined from the commercial world, while others are ex-Researchers who picked up infrastructure skills as part of their career as a researcher, a classic example being the PhD or postdoc who as part of their duties looks after a small Linux cluster for their group, or helps their group deploy their application stack on supercomputers they have access to (or even the Cloud) so that other researchers can get on with their research."
  },
  {
    "objectID": "business/personas/research-infrastructure-developer.html#what-qualifications-or-skills-do-you-need-to-be-a-research-infrastructure-developer",
    "href": "business/personas/research-infrastructure-developer.html#what-qualifications-or-skills-do-you-need-to-be-a-research-infrastructure-developer",
    "title": "Working document AI platform",
    "section": "What qualifications or skills do you need to be a Research Infrastructure Developer?",
    "text": "What qualifications or skills do you need to be a Research Infrastructure Developer?\nThe very nature of this work means that qualifications are hard to come by and so the most important skills are creative problemsolving, a desire to be helpful and the kind of mind that doesn’t accept systems not working properly. Extremely useful technical skills to have (depending on the exact area) are Linux, shell, Python, debugging compiler/library issues, management of schedulers, file-systems and automation tools like Puppet or Ansible."
  },
  {
    "objectID": "business/personas/research-infrastructure-developer.html#challenges-for-research-infrastructure-developers",
    "href": "business/personas/research-infrastructure-developer.html#challenges-for-research-infrastructure-developers",
    "title": "Working document AI platform",
    "section": "Challenges for Research Infrastructure Developers",
    "text": "Challenges for Research Infrastructure Developers\n\nBalancing the needs of the institution vs those of individual academics.\nBuilding infrastructure “right” (not increasing tech debt) despite resource constraints and changing requirements.\nThe technology stack is a moving target and so RIDs need to be flexible and able to keep up to date.\nIT work is not esteemed by institutions or funders and so it can be challenging to feel the value to give to the organisation."
  },
  {
    "objectID": "business/personas/research-infrastructure-developer.html#benefits-of-having-research-infrastructure-developers",
    "href": "business/personas/research-infrastructure-developer.html#benefits-of-having-research-infrastructure-developers",
    "title": "Working document AI platform",
    "section": "Benefits of having Research Infrastructure Developers",
    "text": "Benefits of having Research Infrastructure Developers\n\nThe infrastructure which underlies research is more reliable and sustainable.\nHaving experts build infrastructure frees up “brain capacity” of researchers to focus on research problems.\nSharing of best practices in infrastructure design across projects and the consolidation of hardware and software.\n\n(cl-infrastructure-rid-support)= ## Organisations that support Research Infrastructure Developers\nCurrently there are no national organisations as this is a new area being pioneered at UCL ARC. If you want help setting up a RID team in your organisation please contact ARC.\n\nUCL ARC\n\n(cl-infrastructure-rid-summary)= ## Summary\nResearch Infrastructure Developers are highly skilled, valuable members of any institution that is conducting computational research. They bring technical design skills as well as best practices from hardware/software architecture and open source development to academic research. Some also conduct their own independent research projects."
  },
  {
    "objectID": "index.html#aanleiding-oproep-voor-een-nationaal-actieplan-ai4health",
    "href": "index.html#aanleiding-oproep-voor-een-nationaal-actieplan-ai4health",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Aanleiding: oproep voor een nationaal actieplan AI4health",
    "text": "Aanleiding: oproep voor een nationaal actieplan AI4health\nTijdens de laatste ICT&Health conferentie is een oproep gedaan om te komen tot een nationaal actieplan AI4Health. De kern van deze oproep is dat, ondanks alle lopende initiatieven, er nog steeds veel barrières zijn om data- en AI-gedreven innovaties in de zorg op grote schaal te realiseren. De praktijk wijst uit dat er drie valleys of death overkomen moeten worden (zie Gude, van Eekeren, en Vasseur (2024) voor een recent overzicht):\n\n\n\n\n\n\nFiguur 1: De drie valleys of death die grootschalig gebruik van AI in de zorg in de weg staan.\n\n\n\n\nVan concept tot toegang tot de praktijk. Voordat grootschalig pilots en marktanalyse mogelijk is moet voldaan worden aan de voorwaarden van de Medical Device Regulation (MDR) en de AI act; 80-90% van de innovaties strandt hier. In de verschillende meetings en workshops die NLAIC georganiseerd heeft met zorginstellingen en innovatoren kwam de MDR/AI ACT bij herhaling als grote bottleneck naar boven (altijd top-3).\nVan pilot naar eerste betalende klant. Dit vereist onder meer een solide business case, betrouwbare toegang tot data en professionele ondersteuning. Dit blijft een hardnekkig probleem, niet alleen voor toepassingen in de ziekenhuiszorg maar komt ook terug bij GGZ en VVT. Binnen de ziekenhuiszorg is het adresseren van deze VoD daarom bv een speerpunt voor de SAZ-ziekenhuizen (Expertisecentrum Zorgalgoritmen).\nVan eerste klant naar duurzame opschaling. Implementatie, acceptatie en validatie (“calibratie”) in andere omgevingen dan die van de pilot sites en eerste klant zijn verre van triviaal voor AI-innovaties. De meeste innovaties die de eerste klant weten te bereiken stranden alsnog in deze Valley of Death. Ter verdere illustratie van de huidige stand van zaken: bij een AI-readiness traject van NLAIC kwam naar voren dat nog geen enkele zorginstelling daadwerkelijk AI-ready is.",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#behoefte-aan-een-ai-dataplatform-werktitel-aida",
    "href": "index.html#behoefte-aan-een-ai-dataplatform-werktitel-aida",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Behoefte aan een “AI dataplatform” (werktitel: AIDA)",
    "text": "Behoefte aan een “AI dataplatform” (werktitel: AIDA)\nOp dit moment wordt gewerkt aan een nationaal actieplan om deze barrières te adresseren langs vier actielijnen, te weten\n\nDatabeschikbaarheid\nAI-readiness\nAI-beschikbaarheid\nOrkestratie.\n\nBinnen de actielijn databeschikbaarheid is in de afgelopen jaren het nodige in gang gezet, waaronder het systematisch toepassen van FAIR principes, een helder en breed gedragen ethisch en juridisch kader en het realiseren van gedistribueerde toegang tot data. Tegelijkertijd kunnen we constateren dat het ontbreekt aan voldoende gedetailleerde afspraken om te komen tot een ecosysteem van ‘AI dataplatformen’ waarop effectief onderzoek kan worden gedaan, algoritmes kunnen worden ontwikkeld etc. Een dergelijk platform is een geïntegreerd systeem dat AI-ontwikkelaars ondersteunt met toegang tot data, modellen en andere hulpmiddelen om AI-projecten te ontwikkelen en te verbeteren. Dit platform biedt toegang tot essentiële bronnen, zoals datasets voor het trainen van AI-algoritmen, basis AI-modellen en diensten zoals een ELSA-desk voor ethische en juridische vraagstukken.\nDergelijke ‘AI dataplatformen’ zijn in feite specifieke vormen van beveiligde verwerkingsomgevingen zoals in in de datagovernance verordening artikel 2 lid 20 is gedefinieerd:\n\n“beveiligde verwerkingsomgeving”: de fysieke of virtuele omgeving en organisatorische middelen om te zorgen voor de naleving van het Unierecht, zoals Verordening (EU) 2016/679 (de Algemene verordening gegevensbescherming), met name wat betreft de rechten van datasubjecten, intellectuele-eigendomsrechten, en handels- en statistisch geheim, integriteit en toegankelijkheid, alsook van het toepasselijke nationale recht, en om de entiteit die de beveiligde verwerkingsomgeving biedt in staat te stellen alle gegevensverwerkingsactiviteiten te bepalen en er toezicht op te houden, met inbegrip van het tonen, opslaan, downloaden en exporteren van gegevens en het berekenen van afgeleide gegevens door middel van computeralgoritmen;\n\nOnder de werktitel “AIDA” willen we in de komende periode met experts, belanghebbenden en veldpartijen te komen tot een gecoördineerde realisatie van een dergelijke nutsvoorzieningen. Deze startnotitie en website is bedoeld als interactief discussie document, ter ondersteuning van dit consultatieproces.\nOp dit moment zijn er ontzettend veel ontwikkelingen gaande die relevant zijn voor AIDA. In het onderstaande geven we een samenvatting van relevante initiatieven, waarna we een eerste scoping presenteren en vragen formuleren als start voor de discussie.",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#context-en-relevante-initiatieven",
    "href": "index.html#context-en-relevante-initiatieven",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Context en relevante initiatieven",
    "text": "Context en relevante initiatieven\n\nEuropese initatieven\n\nSimpl Open\nHet Europese Simpl is een “… is an open source, smart and secure middleware platform that supports data access and interoperability among European data spaces.” In januari 2025 zijn gedetailleerde architecturen en functionele beschrijvingen van Simpl-Open opgeleverd om, zijnde een open-source software stack waarmee we deze generieke integratie laag op een gestandaardiseerde manier willen realiseren.\nDe Simpl-Open architectuur is een gedetailleerde uitwerking van bestaande referentie architecturen en is compatible met de Data Spaces Support Center (DSSC) Blueprint (versie 1.5) en de International Data Spaces Reference Architecture Model (IDS-RAM) (huidige versie 4, draft versie 5).\n\n\nAI Factories\nVanuit de EU wordt ingezet op de realisatie van AI Factories, zijnde faciliteiten “… that leverage the supercomputing capacity of the EuroHPC Joint Undertaking to develop trustworthy cutting-edge generative AI models.” Dit initiatief zit meer in de hoek van High Performance Computing (HPC), en wordt ook getrokken vanuit de EuroHPC Joint Undertaking om betrouwbare, state-of-the-art generatieve AI modellen te ontwikkelen.\nSURF is op dit moment penvoerder om namens Nederland een aanvraag in te dienen om een grootschalige Nederlandse AI-faciliteit te realiseren.\n\n\nInvestAI\nEuropa heeft op 11 februari het InvestAI-initiatief aangekondigd om 200 miljard euro aan investeringen te mobiliseren. Dit initatief is o.a. gevoed door CAIRNE, de Confederation of Laboratories for Artificial Intelligence Research in Europe dat al langer pleit voor een CERN voor AI. Op dit moment is het nog onduidelijk wat deze initiatieven concreet zullen betekenen voor AIDA.\n\n\nTEHDAS2\nTEHDAS2 is een “… joint action [that] prepares the ground for the harmonised implementation of the secondary use of health data in the European Health Data Space – EHDS.” Het is een Europees, zorg-specifiek programma, en veel van de werkpakketten zijn direct relevant voor AIDA. Een van de zaken die nader uitgezocht moeten worden is hoe de generieke architectuur van Simpl Open (sector onafhankelijk) zich verhouden tot de ontwerpprincipes en keuzes die binnen TEHDAS2 zijn gemaakt.\n\n\n\nNederlandse context\n\nTwiin\nTwiin is een samenwerkingsverband waarin zorgaanbieders, leveranciers en partners werken aan het Twiin Afsprakenstelsel. Dit Afsprakenstelsel omvat gedetailleerde uitwerkingen over alle lagen van de architectuur voor het beschikbaar maken van gezondheidsgevens. Zo zijn duidelijke keuzes gemaakt om bijvoorbeeld te werken met FHIR-gebaseerde notified pull, het gebruik van BSN voor identificatie, het gebruik van eIDAS voor authenticatie en OAuth2 voor autorisatie.\n\n\nNUTS\nNUTS ontwikkelt en beheert een nutsvoorziening die het delen van zorg-gerelateerde informatie over het Internet mogelijk maakt op een vertrouwelijke, veilige en toegankelijke manier. Het Nuts-netwerk maakt gebruik van internationale standaarden om een vertrouwenslaag op het Internet te realiseren. Die standaarden zijn geïmplementeerd in de Nuts-node: Open Source software die zonder licentiekosten door elke IT leverancier gebruikt kan worden. Leveranciers mogen er ook voor kiezen om zelf de standaarden te implementeren.\n\n\nHealth RI nodes\nLast but certainly not least hebben de Health RI nodes in de afgelopen jaren het nodige ontwikkeld. Binnen de nodes is gewerkt aan verschillende oplossingen en aandachtsgebieden, waaronder het myDRE Trusted Research Environment, het molgenis data platform gericht op wetenschappelijk onderzoek en bioinformatica, het BBMRI-NL beeldanalyse platform om er een paar te noemen.",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#vraagstelling-aida",
    "href": "index.html#vraagstelling-aida",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Vraagstelling AIDA",
    "text": "Vraagstelling AIDA\nOp dit moment worden AI-gedreven innovaties vaak ontwikkeld op niet gestandaardiseerde infrastructuur, wat leidt tot zeer hoge of zelfs onbetaalbare kosten voor een willekeurig onderzoek of innovatie. In analogie zou je kunnen zeggen dat elk onderzoek, elke innovatie zijn eigen terp moet bouwen, alvorens het project daadwerkelijk kan starten. Met AIDA willen we toe naar een Deltawerken voor AI4health. AIDA is hierbij het Deltaplan: het ontwerp van gestandaardiseerde en interoperabele “AI dataplatformen” waarmee komende jaren generieke, landelijk dekkende voorzieningen gerealiseerd kunnen worden. In deze analogie zijn de Deltawerken de realisatie hiervan, waarbij het perspectief is dat we niet naar één AI platform streven, maar een ecosysteem van platformen die interoperabel zijn. Net zoals dat de Deltawerken een ecosysteem van dijken, waterkeringen etc. zijn die elkaar ondersteunen, elk met een eigen functie.\nIn de komende maanden willen we dus een aanzet geven tot het opstellen van het Deltaplan. Belangrijke uitgangspunten en vragen hierbij zijn (indicatief, niet bedoeld als volledige opsomming):\n\nHoe komen we tot harmonisatie, en waar nodig standaardisatie van verschillende oplossingsrichtingen op maximale interoperabiliteit te realiseren.\nHoe kunnen we bestaande en in ontwikkeling zijnde architecturen combineren tot een consistent plan?\nHoe kunnen we middels generieke functies het vertrouwensmodel goed te implementeren?\nHoe kunnen we bestaande initatieven maximale ruimte geven om zelf te blijven doorontwikkelen, met daarbij een minimale set van afspraken\n\nBij het opstellen van deze startnotitie kwam vooral ook naar voren dat er behoefte is aan een eenduidige terminologie en beschrijvingen van componnenten. Dat is een van de concrete deliverables van AIDA voor dit jaar.\nAls start voor de discussie, benoemen we de volgende drie thema’s die we met het veld verder willen verkennen en verdiepen:\n\nOnderscheid verschillende vormen van Secure Processing Environments (SPEs)\nKoppelvlak tussen data en SPEs\nOrchestratie van infrastructuur",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#thema-1-soorten-spes",
    "href": "index.html#thema-1-soorten-spes",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Thema 1: soorten SPEs",
    "text": "Thema 1: soorten SPEs\nVersie 4 van de Health-RI wiki beschrijft de gezondheidsdata-infrastructuur voor onderzoek, beleid en innovatie. Deze infrastructuur is specifiek gericht op secundair gebruik, en is een verbijzondering van de algemene gezamenlijk gezondheidsdata architectuurmodel. Binnen deze architectuur zijn reeds twee soorten van Secure Processing Environments benoemd, namelijk veilige verwerkingsomgevingen) en gefedereerde verwerkingsomgevingen.\n\n\n\n\n\n\nFiguur 2: Conceptuele architectuur voor secundair gebruik\n\n\n\nDe basis gedachte achter AIDA is dat er verschillende soorten van secure processing environments (SPEs) zullen zijn. Daarbij introduceren we een derde type, de hybride SPE. In dit document hanteren we de namen van deze drie typen om expliciet onderscheid te maken; we zullen spreken van SPEs in het algemeen als we alle drie de soorten bedoelen. We geven een korte schets en voorbeelden van elk type.\n\n\n\nTabel 1: Drie soorten van Secure Processing Environments die binnen de scope van AIDA vallen.\n\n\n\n\n\n\n\n\n\nCentrale SPE\n\nvaak benoemd als Trusted Research Environment\nveel bestaande voorbeelden, zie o.a. EOSC-ENTRUST\nmachine learning op tabulaire data mogelijk\nstatistical disclosure control op output\n\n\n\nDecentrale SPEs\n\ndecentrale c.q. federated benadering\noorspronkelijk bedoelt voor machine learning\nkan ook gebruikt worden voor statistische analyse\nmoeilijker om mee te werken\n\n\n\nHybride SPE (H-SPE)\n\nCombinatie van bovenstaande technieken\nNodig om gebruik te maken centrale rekencapaciteit\nGedachte om gebruiksgemak te verbeteren\n\n\n\n\n\n\n\n\nCentrale: SURF Secure Analysis Environment (SANE)\nSURF Secure ANalysis Environment (SANE) is een virtuele, volledig afgeschermde omgeving waarop met vooraf goedgekeurde analyse software draait en toegang tot sensitive data wordt gegeven (Figuur 3). In onderstaand overzicht is SANE gepositioneerd als TRE, waarmee de data aanbieder controle houdt over de data die ter beschikking wordt gesteld en waarmee de data consumer op een makkelijke manier toegang krijgt. SANE biedt functionaliteiten op het gebied van Research Analytics, Secure Data Zone en Data Discovery. Meer details staan in de blauwdruk van EOSC-ENTRUST.\n\n\n\n\n\n\nFiguur 3: Positionering van SANE binnen een generieke data space architectuur.\n\n\n\nBelangrijk kenmerk van SANE en andere TREs is dat de data fysiek naar de Secure Data Zone wordt gekopieerd. Naast het veilig aanbieden van data (als data houder), is dit ook het mechanisme waarmee data gebruikers hun eigen data mee kunnen nemen naar de TRE, om daarbinnen te koppelen aan andere data. Dit gebeurd vaak met gebruik van pseudonimisering. De CBS microdata omgeving werkt op een vergelijkbare manier.\nBinnen de blauwdruk van EOSC-ENTRUST wordt gesproken over Federation Services tussen verschillende TREs. Daarbij gaat het om data federation: data wordt (tijdelijk) van de ene naar de andere TRE gekopieerd zodat het daar in combinatie verwerkt kan worden. Data federation als mechanisme is anders dan federated learning: daarbij worden de berekeningen decentraal uitgevoerd en alleen de resultaten centraal gedeeld (zie hieronder). Federated learning is met name nuttig voor horizontaal gepartitioneerde data. Voor verticaal gepartitioneerde data, is data federation zoals beschreven door EOSC-ENTRUST meer geschikt.\nEr zijn meer voorbeelden van centrale SPEs. Zo hebben de meeste National Statistics Offices (NSOs) zoals het CBS een microdata omgeving. Alhoewel deze omgevingen zijn opzet voordat machine learning zijn intrede deed, bieden de meeste microdata omgevingen nu ook al de mogelijkheid om ‘lichte’ algoritmes te trainen op tabulaire data. Een rapport van de Verenigde Naties beschrijft dat deze omgevingen in toenemende mate ook worden uitgebreid met nieuwe AI-technieken, zoals privacy-enhancing technologieën (PETs, zie ‘The PET Guide’ (2023)).\nEr zijn ook voorbeelden van centrale SPEs specifiek voor de zorg:\n\nHet Finse Social and Health Data Permit Authority (Findata) biedt met Kapseli een landelijke voorziening aan dat aanvullend is op het Finse NSO.\nHet Mayo Clinic Platform_Discover is een voorbeeld van een platform binnen een netwerk van zorg leveranciers.\n\n\n\nDecentrale SPEs: PLUGIN/vantage6\nDecentrale SPEs maken gebruik van federated learning (FL), wat als concept ook wel bekend staat als de Personal Health Train (PHT). FL wordt in toenemende mate gebruikt in de zorg (Teo e.a. 2024); de term FL wordt vooral gebruikt om naar het technische concept te verwijzen, terwijl PHT verder gaat in het definiëren van afspraken rondom het gebruik van FL. In Nederland is een actieve community rondom het vantage6 platform dat wordt gebruikt in het PLUGIN project, en internationaal in 50 andere netwerken. Het basis principe is dat bij FL de gegevens op afzonderlijke ‘data stations’ verschillende apparaten blijven die participeren in het federatieve netwerk. Om deze data te gebruiken voor machine learning, wordt bij elk data station het algoritme lokaal c.q. afzonderlijk getraind. Vervolgens worden alleen de resultaten van het algoritme - bijvoorbeeld geaggregeerde statistieken of de modelparameters van het neurale netwerk - gedeeld met een centrale server. Deze server combineert de resultaten van afzonderlijke modellen tot één model, welke vervolgens met alle deelnemers van het federated SPE gedeeld wordt.\n\n\n\n\n\n\nFiguur 4: Overzicht van vantage6 infrastructuur zoals in PLUGIN is gerealiseerd.\n\n\n\nHet PLUGIN project heeft een decentrale SPE van tientallen ziekenhuizen gerealiseerd, waarbij gebruik wordt gemaakt vantage6 als platform. Belangrijkste kenmerken van deze opzet zijn:\n\nGebruik van beveiligde containers en virtual private networks voor de infrastructuur laag;\nOntzorgen van deelnemende ziekenhuizen, waarbij gebruik wordt gemaakt van een generieke Linux server in het IT domein van het ziekenhuis dat als basis dient om de opslag en rekenkracht om het vantage6 netwerk te realiseren. Afhankelijk of een ziekenhuis mee doet als trainingsziekenhuis of alleen als gebruiker dient een zwaardere resp. lichtere Linux server te worden geconfigureerd;\nVoor elk project wordt de berekening c.q. machine learning expliciet ‘verpakt’ in een Docker container, zijnde de berekening die daadwerkelijk wordt uitgevoerd;\nDe generieke Linux server wordt ook gebruikt om dashboard, informatieproducten etc. te hosten binnen het IT domein van het ziekenhuis.\n\nHet gebruik van een standaard data model (op de data stations) is een belangrijke randvoorwaarde om FL te kunnen doen. Naast het gebruik van vantage6 als kerntechnologie, heeft PLUGIN ervoor gekozen om FHIR als data standaard te gebruiken. Hiertoe is een FHIR profiel in ontwikkeling die aansluit op de bestaande ZIBS2020 bouwstenen. Meer achtergrond over de keuze voor FHIR is te lezen in dit artikel. Andere voorbeelden van decentrale SPEs gebaseerd op FL zijn hier te vinden.\n\n\nHybride SPE: UbiOps en Roseman Labs\n\n\n\n\n\n\n\n\n\n\n\n\nFiguur 5: Hybride SPE in de veiligheidsketen tussen verschillende Security Operating Centra (SOCs).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiguur 6: UbiOps orchestratielaag\n\n\n\n\n\n\nDe hybride SPE is een nieuwe oplossingsrichting die we willen verkennen en realiseren in AIDA. Er zijn minder concrete voorbeelden van een dergelijke opzet. UbiOps en Roseman Labs hebben een oplossing die er het dichts bij in de buurt komt (Figuur 5). In analogie met data spaces, gaat het hier om het verbinden van verschillende Security Operating Centra (SOC) in de beveiligingsketen. In een hybride SPE kunnen compute (rekenkracht) en storage (opslag) zowel lokaal als centraal worden uitgevoerd. In deze architectuur worden bijvoorbeeld pre-processing van data decentraal uitgevoerd in de SOCs in de onderste laag van Figuur 5. De resultaten van deze pre-processing gaan naar het centrale platform, in dit geval het NCSC. Daar kunnen vervolgens ook weer (vervolg-)berekeningen worden uitgevoerd, op de storage en compute die beschikbaar zijn in de omgeving van de NCSC.\nDe hybride SPE is mogelijk gemaakt door UbiOps, een platform leverancier die de orkestratielaag biedt waarmee alle storage en compute centraal wordt beheerd (Figuur 6). Een belangrijk ontwerpprincipe van deze orkestratielaag is dat het verschillende soort fysieke infrastructuur kan managen, variërend van bare metal servers, Kubernetes cluster, virtuele machines, public cloud infrastructuur etc.\nEen ander onderscheidende kenmerk van deze opzet is dat de centrale dataverwerking ook onder encryptie uitgevoerd kan worden via het Roseman Labs MPC (Multiparty Computation) platform. Door berekeningen in-the-blind uit te voeren, zijn de data extra beschermd.\nDeze opzet van hybride SPE is in lijn met de recent gepubliceerde Simpl-Open architectuur. Deze aanpak biedt de mogelijkheid om over verschillende SPEs tot harmonisatie en interoperabiliteit te komen. Denk bijvoorbeeld aan een situaties waarbij een analyse kan worden uitgevoerd over verschillende Health-RI nodes heen. In Paragraaf 7 gaan we hier dieper op in.\n\n\nStrategy view op AIDA\n\n\n\n\n\n\nFiguur 7: De Strategy view als startpunt voor de discussie.)\n\n\n\nGegeven deze verschillende soorten SPEs is een eerste strategy view van AIDA geschetst in Figuur 7. De value stream elementen zijn beschreven in termen van het ontwikkelproces van CRISP-DM. Deze value stream kan worden gerealiseerd met behulp van verschillende soorten SPEs. De modulaire capabilities zijn de verschillende functionele bouwblokken die in een SPE gebundeld/aangeboden kunnen worden. De gedachte is dat elke SPE, afhankelijk van de context, doelgroep etc. een eigen configuratie van capabilities heeft.",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#thema-2-koppelvlak-datastores-en-spe",
    "href": "index.html#thema-2-koppelvlak-datastores-en-spe",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Thema 2: koppelvlak datastores en SPE",
    "text": "Thema 2: koppelvlak datastores en SPE\nHet ontwerp van AIDA vereist harmonisatie van de koppelvlakken (component 9 in Figuur 2) tussen de datastores (component 7) en de SPE. Binnen de Simpl Open architectuur wordt gesproken van Resource discovery (zie Figuur 7, linker cluster van capabilities). Versie 4 van de Health-RI wiki geeft een uitwerking van deze discovery services in een aantal bouwblokken (Figuur 8) . In het onderstaande gaan we in op een aantal van deze bouwblokken die relevant zijn voor de uitwerking van AIDA\n\n\n\n\n\n\nFiguur 8: Bouwblokken voor geautomatiseerd proces voor beheer, uitgifte en gebruik van de data (bron.\n\n\n\n\nOnderscheid datastores en datasets\nEen van de uitgangspunten van de Health-RI architectuur is dat data zo dicht mogelijk bij de bron FAIR wordt gemaakt. Het concept van een FAIR Data Point is hierin leidend, waarvan inmiddels verschillende implementaties zijn en complementaire software componenten voor b.v. het harvesten van FAIR metadata catalogi.\nEen van de zaken die nader gespecificeerd moet worden is het onderscheid tussen een FAIR data set en een datastore. Een datastore in deze is een component dat als system of record data beschikbaar stelt voor hergebruik en/of een longitudinale datastore waarbij data vanuit verschillende bronsystemen worden gecombineerd en gepersisteerd. openEHR en OMOP zijn veel gebruikte standaarden voor de implemenetatie van een dergelijk datastore. Voor meer details en achtergrond verwijzen we naar Tsafnat e.a. (2024).\nMet name voor het beschikbaar stellen en hergebruiken van tabulaire data uit EPDs, HISen etc. is het wenselijk om snel te verkennen of een bepaalde dataset relevant is voor een gebruiker. We zien ‘verkenner’ voor ons, waarbij potentiele data gebruikers in staat worden gesteld om de datastore te queryen. Typisch betreft dit het zoeken naar relevante populaties in de datastores, bijvoorbeeld “hoeveel patienten ouder dan 65 met aandoening XXX” zitten in de datastore. De resultaat van een dergelijk query is overigens altijd een anonieme dataset met enkel geaggregeerde, beschrijvende statistieken.\nDe meeste - en zo niet alle - open standaarden voor datastores hebben een mechanisme om dit te ondersteunen:\n\nopenEHR gebruikt hiervoor het Archtetype Query Language (AQL)\nOMOP gebruikt SQL (geen OMOP-specifieke query language) direct op de relationale database\nbinnen het FHIR ecosysteem zijn nieuwe standaarden zoals Bulk FHIR en SQL-on-FHIR bedoelt om subsets te definiëren en op een makkelijke manier data te aggregeren\n\nDeze benadering zorgt ervoor dat (potentiële) data gebruikers op een eenduidige manier ‘inclusiecriteria’ kunnen bepalen op de datastore. Het gaat hierbij om inclusie van rijen (subjecten, patient) en kolommen (welke attributen, waarden). Alhoewel het runnen van de query elke keer een iets ander resultaat geeft (je kunt meer rijen krijgen omdat nieuwe patiënten zijn toegevoegd), is het zodanig gestandaardiseerd dat het een vergelijkbare functie vervult als een statische, gepersisteerde dataset zoals een FDP dat beoogt. De metadatering van de query op de datastore zal waarschijnlijk met nagenoeg dezelfde metadata standaarden (DCAT-AP) kunnen worden gedaan.\nAlhoewel de inrichting van een ‘verkenner’ functie technisch mogelijk is, blijkt uit de praktijk dat er nog het nodige aan data verificatie, mapping etc. gedaan moet worden om te komen tot een ‘bevroren’ dataset die geschikt is voor secundair gebruik. Tijdens de hackathon in december 2024 zijn een aantal knelpunten geidentificeerd. In de uitwerken van AIDA willen we expliciet ingaan hoe we dit kunnen oplossen.\n\n\nStandaard voor koppelen van verticaal gepartitioneerde data middels pseudonimisering\nIn veel use-cases dient data uit verschillende bronnen te worden gekoppeld op subject/patient niveau. Op dit moment is ZorgTTP een veelgebruikte pseudonimiseringsmethode dat wordt gebruikt, dat sinds 2015 tevens openbaar is. Het gebruik van pseudoniemen om te koppelen is met name relevant voor centrale en hybride SPEs: op deze manier kan een integrale dataset gemaakt worden voor secundair gebruik. Een decentrale SPE is minder geschikt voor het koppelen van zogenaamde verticaal gepartitioneerde data; het is niet onmogelijk maar zeker bewerkelijker.",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#sec-orchestratie",
    "href": "index.html#sec-orchestratie",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Thema 3: Orchestratie van infrastructuur",
    "text": "Thema 3: Orchestratie van infrastructuur\nOrchestratie van infrastructuur binnen en tussen verschillende AIDA platformen is essentieel om tot een samenhangend Deltawerken te komen. Binnen de Simpl Open architectuur is de Infrastructure Layer ook in detail uitwerkt.\n\n\n\n\n\n\nInfrastructure Layer in Simpl Open\n\n\n\nThe capabilities provided by this layer enable the consumers to easily provision the necessary computing and storage resources to execute their workloads in a secure and energy-efficient way. The infrastructure orchestration, automates the provisioning of the infrastructure resources to enable the various infrastructure providers to interconnect infrastructure orchestration and get exposed via a standard interface. The distributed execution, allows the consumer to deploy applications and execute computations close to the distributed execution data.\nThe cloud & edge computing capability provides the opportunity to provision various resources to execute computations or store data in the environment of their choice. The platform-as-a-service building blocks provide several database engines and other platform-level resources. Finally, the HPC capability permits the consumer to perform complex calculations at high speed by providing a cluster of high-performance computers.\nThe infrastructure building blocks can be easily combined with each other to create even more value for the consumer. For instance, after successfully analysing certain data with the help of the provisioned platform-as-a-service analytical resources or the HPC capability, the consumer may want to store the used datasets and/or the results of their calculations. In this case, the PaaS storage building block can easily fulfil the storage needs of the end user. In case a consumer would like to develop a stand-alone application, they may also use various PaaS resources at the same time. They can leverage the different storage options to store each sort of data in the most efficient manner (e.g. the transactional data in a transactional database, while the sensor data in a NoSQL database). Besides, they can use the cloud & egde computing capabilities to deploy and run their applications, and the distributed execution capability even enables them to run the code close to the edge.\n\n\n\n\n\nConceptueel overzich Simpl Open, met een centrale plek van de Infrastructure Layer\n\n\nSimpl Open hanteert de volgende ontwerpprincipes voor de infrastructuur laag:\n\nHet moet mogelijkheid zijn om over verschillende fysieke locaties een data space op te zetten\nHet gebruik van agents als mechanisme voor het orkestreren van allerlei compute en storage binnen een data space en tussen een data space\nIndeling van twee Tiers voor Identificatie, Authenticatie en Autorisatie (IAA) functies, waarvoor standaarden gebruikt moeten worden\n\nTier 1: IAA van gebruikers\nTier 2: IAA voor machine-to-machine orkestratie\n\n\nNaast gebruik van catalogi voor data en applicaties wordt ook het gebruik van een infrastructuur voorgeschreven, zodat daarmee inzichtelijk is welke compute en storage beschikbaar is binnen het netwerk\nAansluiten bij bestaande standaarden en open source componenten zoals Keycloak als IAA applicatie component, OAUth2 voor autorisatie, S3-compatible blob storage als default opslag dienst en Kubernetes voor de containerinfrastructuur\n\nDe modulaire architectuur van Simpl Open heeft veel raakvlakken met de huidige trend binnen de data engineering community om toe te werken naar een zogenaamde composable data stack.1 In deze composable data stack worden storage, compute en query languages volledige ‘ontvlochten’ in afzonderlijke componenten. Interoperabiliteit tussen de componenten is gebaseerd om moderne standaarden zoals Apache Arrow, Apache Parquet, Apache Iceberg en Apache Substrait (zie Figuur 9).\n\n\n\n\n\n\nFiguur 9: Vereenvoudigd overzicht van de composable data stack (bron).",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Van terpen naar deltawerken voor AI in de zorg",
    "section": "Voetnoten",
    "text": "Voetnoten\n\n\nZie de Composable Codex voor een uitgebreide toelichting.↩︎",
    "crumbs": [
      "Startnotitie"
    ]
  },
  {
    "objectID": "strategisch/ai-in-de-zorg.html#definitie-van-ai-en-machine-learning-in-de-zorg",
    "href": "strategisch/ai-in-de-zorg.html#definitie-van-ai-en-machine-learning-in-de-zorg",
    "title": "Strategisch perspectief",
    "section": "Definitie van AI en machine learning in de zorg",
    "text": "Definitie van AI en machine learning in de zorg\nVolgens de OECD en de AI Verordening is een AI systeem:\n\n… a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.\n\nen dus in het Nederlands\n\n… een machine gebaseerd systeem dat is ontworpen om met verschillende niveaus van autonomie te werken en dat na het inzetten ervan aanpassingsvermogen kan vertonen, en dat, voor expliciete of impliciete doelstellingen, uit de ontvangen input afleidt hoe output te genereren zoals voorspellingen, inhoud, aanbevelingen of beslissingen die van invloed kunnen zijn op fysieke of virtuele omgevingen.\n\nDeze definitie is erg breed. Om de scope van AIDA te bepalen maken we onderscheid tussen AI en machine learning, zoals in Figuur 1. Voor nu richten we ons op machine learning en vergelijkbare vormen van statistical learning. We plaatsen expliciet de ontwikkeling van autonome systeem (robotica) en reinforcement learning buiten scope van AIDA.\n\n\n\n\n\n\nFiguur 1: Versimpelde indeling van AI en machine learning\n\n\n\nBinnen de zorg wordt vaak een onderscheid gemaakt tussen klinische en niet-klinische (lees: bedrijfsmatige) toepassingen van AI\n\nKlinische machine learning toepassingen\n\n\n\n\n\n\nFiguur 2\n\n\n\n\nScope van de Leidraad AI in de zorg AI Prediction Algorithm (AIPA): voorspelling over (toekomstige) gezondheidstoestand\nValt vaak in hoog risico categorie (zie stroomschema Figuur 2)\n\n\n\nMachine learning voor bedrijfsmatige toepassingen (niet klinisch)\n\nLogistieke optimalisatie:\n\nvoorspellen van ontslagmoment intensive care (Pacmed Critical)\nvoorspellen ligduur spoedeisende hulp (EZA)\nvoorspellen no-shows polikliniek (ErasmusMC)\n\nVerminderen van administratie- en registratielast\n\nEnorme groei in gebruik generatieve AI\nLLMs voor NLP taken (Sensire, ETZ)\nSpeech-to-text (Autoscriber, Juvoly)\nVrije tekst omzetten in gestructureerde data (Healthsage AI)\nAI ondersteund coderen (DHD)\n\n\n\n\nBenodigde resources van verschillende AI toepassingen\ntabulaire data –&gt; beelden –&gt; NLP (met pre-trained modellen) –&gt; trainen fondationational GenAI models, Omics\nook relateren aan verwachte aantal gebruikers/use-cases",
    "crumbs": [
      "Strategisch perspectief",
      "AI in de zorg"
    ]
  },
  {
    "objectID": "strategisch/ai-in-de-zorg.html#het-standaard-ontwikkelproces-voor-machine-learning-crisp-dm",
    "href": "strategisch/ai-in-de-zorg.html#het-standaard-ontwikkelproces-voor-machine-learning-crisp-dm",
    "title": "Strategisch perspectief",
    "section": "Het standaard ontwikkelproces voor machine learning: CRISP-DM",
    "text": "Het standaard ontwikkelproces voor machine learning: CRISP-DM\nHet ontwikkelen, trainen en evalueren van machine learning algoritmes wordt gedaan aan de hand van een standaard proces. De CRISP-DM methode is hiervan de meest gebruikte (zie Figuur 3). de Mast e.a. (2022) beschrijft hoe andere procesmodellen voor problem solving zoals Six Sigma DMAIC and PDCA ook hiervoor gebruikt kunnen worden. Aan de hand van dit proces schetsen wij hoog over wat AIDA zoal moet kunnen ondersteunen\n\n\n\n\n\n\nFiguur 3: CRISP-DM methode voor het ontwikkelen van machine learning algoritmes.\n\n\n\n\nExploratieve data analyse (EDA)\nOmvat eerste twee fases, Business Understanding en Data Understanding. Een onderzoeker of een innovator moet kunnen ‘bladeren’ door de beschikbare data. Een van de resultaten van deze fase is dat een dataset wordt gedefinieerd, wat onderscheidend kenmerk is t.o.v. algemene data beschikbaarheid (primair gebruik). In verder ontwikkeling van AI is het essentieel dat we een dataset moeten kunnen ‘bevriezen’, annoteren en FAIR geschikt maken voor hergebruik door andere onderzoekers of voor responsible AI/goedkeuring van een algoritme.\nDe exploratieve data analyse heeft een belangrijk raak-/koppelvlak met het domein van databeschikbaarheid. Data gebruikers moeten ondersteund worden om te kunnen verkennen welke data beschikbaar is, en hoe met de aangeboden data gewerkt kan (en moet) worden. Binnen AIDA zullen hiervoor richtlijnen en/of standaarden worden opgezet.\n\n\nData Preparation en Modeling\nDit is de kern van het machine learning ontwikkeling. Veel verschillende libraries moeten beschikbaar zijn omdat er veel verschillende use-cases zijn, dus b.v.:\n\ndata preparation: allerlei verschillende soorten ‘data wrangling’ libraries, afhankelijk van de voorkeuren van de gebruiker\nfeature stores en vector databases: voor het ontwikkelen van grote modellen wordt gebruik gemaakt van intermediare respresentaties van de data. Tekst wordt b.v. omgezet naar vector embeddings. Deze wil je niet elke keer opnieuw willen bereken, dus er is specifieke (tijdelijke) opslag nodig\nverschillen machine learning libraries, zoals tabulaire machine learning (tree-based methods, tijdsreeksen) of deep learning (tensorflow, pytorch)\n\n\n\nEvaluation\nDit is direct gerelateerd aan de overgang van pilot naar eerste klant. Een technisch correct algoritme zal een langer proces van validatie in de klinische praktijk moeten doorgaan, alvorens het breed uitgerold kan worden. AIDA dient mogelijkheden te bieden voor het verzamelen en bewaren van feedback uit field trials over het gebruik van ontwikkelde algoritmes.\n\n\nDeployment\nIdealiter biedt een landelijke voorziening als AIDA ook de mogelijkheid om een getraind model te hosten, zoals bijvoorbeeld Evidencio dat nu biedt voor (kleine) modellen voor medical decision support",
    "crumbs": [
      "Strategisch perspectief",
      "AI in de zorg"
    ]
  },
  {
    "objectID": "strategisch/ai-in-de-zorg.html#scope-aida",
    "href": "strategisch/ai-in-de-zorg.html#scope-aida",
    "title": "Strategisch perspectief",
    "section": "Scope AIDA",
    "text": "Scope AIDA\n\nOndersteunen beide soorten. Voor een uitgebreider overzicht, zie Ali e.a. (2023)\nUitdaging is dat afhankelijk van de soort machine learning dat ontwikkeld wordt, de functionele eisen van AIDA anders zijn. Concreet: het trainen van een relatief eenvoudige classificatie model op een dataset van 10.000 patienten vereist veel minder rekenkracht dan b.v. het trainen van een foundational LLM met gebruik van klinische teksten uit EPD\nMachine learning vatten we breed op, dus het omvat ook andere vormen van statistical learning en problem solving (zie de Mast e.a. (2022)). Naast machine learning dient AIDA ook gebruikt te kunnen worden voor\n\nCausal inference, Bayesian inference, structural causal modeling\nDeductieve modellen, zoals vaak in operations research worden gebruikt",
    "crumbs": [
      "Strategisch perspectief",
      "AI in de zorg"
    ]
  },
  {
    "objectID": "strategisch/index.html",
    "href": "strategisch/index.html",
    "title": "Strategisch perspectief",
    "section": "",
    "text": "Amsterdam Health Data Space\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScoping review bestaande blauwdrukken\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategisch perspectief\n\n\nDefinities van AI in de zorg en het ontwikkelproces van AI\n\n\nWe maken een onderscheid tussen AI en machine learning, en tussen klinische en niet-klinische toepassingen van machine learning in de zorg. We introduceren CRISP-DM als het standaard proces voor het ontwikkelen van machine learning algoritmes. Deze definities zijn het startpunt voor de verhaallijn van AIDA. \n\n\n\n\n\n\n\n\n\nGeen overeenkomsten",
    "crumbs": [
      "Strategisch perspectief"
    ]
  },
  {
    "objectID": "strategisch/ahds.html",
    "href": "strategisch/ahds.html",
    "title": "Amsterdam Health Data Space",
    "section": "",
    "text": "Toelichting wat Amsterdam al heeft."
  },
  {
    "objectID": "strategisch/blauwdrukken.html",
    "href": "strategisch/blauwdrukken.html",
    "title": "Scoping review bestaande blauwdrukken",
    "section": "",
    "text": "Er zijn verschillende referentie architecturen en blauwdrukken die vergelijkbaar zijn met AIDA. Wij kiezen ervoor om de Simpl Open architectuur in combinatie met de Blueprint 1.5 van de Data Spaces Support Centre (DSSC) als startpunt te gebruiken. De Simpl Open architectuur is het meest gedetailleerd en richt zich met name ook op de orchestratie van de infrastructuur, wat essentieel is om hybride SPEs te realiseren. De DSSC Blueprint is met name nuttig als framework om de concepten te communiceren. Het wordt ondersteund door een grote community en het expliciet bedoelt is om aan de hand van bouwblokken specifieke ontwerpen en implementaties te maken. Figuur 1 geeft een overzicht van de bouwblokken die zijn onderverdeelt in bedrijfsmatige en organisatie bouwblokken, en technische bouwblokken.De online documentatie geeft een gedetailleerde beschrijving van elke bouwblok. Daarnaast is er een uitgebreide DSSC Glossary die wij zullen gebruiken voor het beschrijven en definiëren van AIDA. Daarbij hanteren we de methode zoals in Figuur 2 is geïllustreerd, namelijk dat elke SPE een verzameling van bouwblokken is, waarvoor we (met de tijd) verschillende implementatie en configuraties zullen verwachten.\n\n\n\n\n\n\nFiguur 1: DSSC Blueprint 1.5 Building Block\n\n\n\n\n\n\n\n\n\nFiguur 2: DSSC\n\n\n\nDe beoogde functionaliteit van AIDA valt onder de categorie Value Creation Services. Figuur 3 geeft een gedetailleerder overzicht welke soort functionaliteit hieronder wordt verstaan.\n\n\n\n\n\n\nFiguur 3: Overzicht van capabilities binnen Value Creation Services",
    "crumbs": [
      "Strategisch perspectief",
      "Vergelijking blauwdrukken"
    ]
  },
  {
    "objectID": "strategisch/blauwdrukken.html#dssc-blueprint-1.5-als-startpunt",
    "href": "strategisch/blauwdrukken.html#dssc-blueprint-1.5-als-startpunt",
    "title": "Scoping review bestaande blauwdrukken",
    "section": "",
    "text": "Er zijn verschillende referentie architecturen en blauwdrukken die vergelijkbaar zijn met AIDA. Wij kiezen ervoor om de Simpl Open architectuur in combinatie met de Blueprint 1.5 van de Data Spaces Support Centre (DSSC) als startpunt te gebruiken. De Simpl Open architectuur is het meest gedetailleerd en richt zich met name ook op de orchestratie van de infrastructuur, wat essentieel is om hybride SPEs te realiseren. De DSSC Blueprint is met name nuttig als framework om de concepten te communiceren. Het wordt ondersteund door een grote community en het expliciet bedoelt is om aan de hand van bouwblokken specifieke ontwerpen en implementaties te maken. Figuur 1 geeft een overzicht van de bouwblokken die zijn onderverdeelt in bedrijfsmatige en organisatie bouwblokken, en technische bouwblokken.De online documentatie geeft een gedetailleerde beschrijving van elke bouwblok. Daarnaast is er een uitgebreide DSSC Glossary die wij zullen gebruiken voor het beschrijven en definiëren van AIDA. Daarbij hanteren we de methode zoals in Figuur 2 is geïllustreerd, namelijk dat elke SPE een verzameling van bouwblokken is, waarvoor we (met de tijd) verschillende implementatie en configuraties zullen verwachten.\n\n\n\n\n\n\nFiguur 1: DSSC Blueprint 1.5 Building Block\n\n\n\n\n\n\n\n\n\nFiguur 2: DSSC\n\n\n\nDe beoogde functionaliteit van AIDA valt onder de categorie Value Creation Services. Figuur 3 geeft een gedetailleerder overzicht welke soort functionaliteit hieronder wordt verstaan.\n\n\n\n\n\n\nFiguur 3: Overzicht van capabilities binnen Value Creation Services",
    "crumbs": [
      "Strategisch perspectief",
      "Vergelijking blauwdrukken"
    ]
  },
  {
    "objectID": "strategisch/blauwdrukken.html#idsa",
    "href": "strategisch/blauwdrukken.html#idsa",
    "title": "Scoping review bestaande blauwdrukken",
    "section": "IDSA",
    "text": "IDSA",
    "crumbs": [
      "Strategisch perspectief",
      "Vergelijking blauwdrukken"
    ]
  },
  {
    "objectID": "strategisch/blauwdrukken.html#eosc-entrust",
    "href": "strategisch/blauwdrukken.html#eosc-entrust",
    "title": "Scoping review bestaande blauwdrukken",
    "section": "EOSC-Entrust",
    "text": "EOSC-Entrust\n\nEOSC-ENTRUST aims to create a European network of Trusted Research Environments (TREs) for sensitive data and drive European interoperability between TREs by development of a common blueprint for federated data access and analysis – the EOSC-ENTRUST Blueprint & Interoperability Framework (ENTRUST Blueprint, for short). The final ENTRUST Blueprint will consist of template legal agreements, architecture specifications, operating procedures, interface definitions, and a glossary of terminologies. This document is the first version of the ENTRUST Blueprint and presents a draft architecture specification and glossary.\n\nDe eerste versie van de blauwdruk is in december 2024 gepubliceerd (link). Een eerste versie van de architectuur is opgesteld met de DARE UK Federated Architecture Blueprint als uitgangspunt, wat vervolgens in vergeleken en aangevuld met NOTRE (Norwegian TREs), het Nederlandse SURF Secure ANalysis Environment (SANE) en het Finse CSC Secure Data Services.\n\n\n\nENTRUST infrastructuur architectuur (versie 1, december 2024)\n\n\nVeel componenten van de ENTRUST architectuur zijn een op een te mappen op de DSSC Blueprint. Meest in het oog springende is dat ENTRUST vooral richt op gefedereerde TREs.",
    "crumbs": [
      "Strategisch perspectief",
      "Vergelijking blauwdrukken"
    ]
  },
  {
    "objectID": "strategisch/blauwdrukken.html#europe-ai-factory",
    "href": "strategisch/blauwdrukken.html#europe-ai-factory",
    "title": "Scoping review bestaande blauwdrukken",
    "section": "Europe AI Factory",
    "text": "Europe AI Factory",
    "crumbs": [
      "Strategisch perspectief",
      "Vergelijking blauwdrukken"
    ]
  },
  {
    "objectID": "strategisch/blauwdrukken.html#bdva-reference-model",
    "href": "strategisch/blauwdrukken.html#bdva-reference-model",
    "title": "Scoping review bestaande blauwdrukken",
    "section": "BDVA reference model",
    "text": "BDVA reference model\n\n\n\n\n\n\nFiguur 4: Big Data Value Reference Model, Curry e.a. (2021)\n\n\n\nIndeling naar soorten data waarmee binnen AIDA gewerkt kan worden is handig:\n\nTabulaire data (gestructureerde data): zowel klinisch (vanuit EPD) als ook bedrijfsmatige data\ntijdreeksen: hoogfrequente klinische data zoals bijvoorbeeld EEG. Vereisen specifieke opslagformaten en standaardisatie vanwege omvang.\ngeospatial data: bijvoorbeeld voor pandemic response en geoanalyses\nbeeld, video en geluid: veel gebruikt voor diagnostiek\ntekst en genome data: sequentiele data waarmee via LLMs steeds meer gedaan wordt\ngraaf-gebaseerde data: relevant voor klinische ontologieen, vastleggen van metadata en het automatische transformeren van data tussen verschillende formaten\n\nVan de benoemde functionele domeinen zijn de volgende in scope van AIDA:\n\nhorizontaal: de primaire processen die onderdeel zijn van het AI ontwikkelproces\n\ndata visualisatie en user interaction:\ndata analyse\ndata processing architectuur\n\nverticaal: ondersteunende, generieke functies\n\ndevelopment, engineering & DevOps\nstandaarden",
    "crumbs": [
      "Strategisch perspectief",
      "Vergelijking blauwdrukken"
    ]
  },
  {
    "objectID": "technisch/index.html",
    "href": "technisch/index.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "hier komen software-neutrale specificaties.",
    "crumbs": [
      "Applicaties en technisch perspectief"
    ]
  },
  {
    "objectID": "technisch/index.html#technische-uitwerking",
    "href": "technisch/index.html#technische-uitwerking",
    "title": "Working document AI platform",
    "section": "",
    "text": "hier komen software-neutrale specificaties.",
    "crumbs": [
      "Applicaties en technisch perspectief"
    ]
  },
  {
    "objectID": "business/personas/index.html",
    "href": "business/personas/index.html",
    "title": "Personas",
    "section": "",
    "text": "Vanuit de dagelijkse praktijk van data science & AI ontwikkeling weten we dat er verschillende rollen zijn. Deze rollen zijn echter nog fluide, er is geen ‘standaard’ definitie. Als startpunt nemen we de beschrijvingen van personas van The Turing Way, een Europees initatief dat best practices beschrijft data science in een onderzoek- en ontwikkelomgeving. Voor nu zijn de Engelse teksten overgenomen, deze kunnen later naar het nederlands worden vertaald.\nDe rollen die binnen de architectuur van databeschikbaarheid reeds zijn beschreven zijn buiten beschouwing gelaten (Rollen v4.0)\n\n\n   \n     \n     \n       Sorteer op\n       Standaard\n         \n          Titel\n        \n         \n          Auteur\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitel\n\n\nAuteur\n\n\n\n\n\n\nCommunity Managers: Overview\n\n\n \n\n\n\n\nData Wranglers: Overview\n\n\n \n\n\n\n\nResearch Application Managers: Overview\n\n\n \n\n\n\n\nResearch Infrastructure Developer: Overview\n\n\n \n\n\n\n\nResearch Software Engineer: Overview\n\n\n \n\n\n\n\nResearch Software Engineering Personal Story\n\n\n \n\n\n\n\n\nGeen overeenkomsten",
    "crumbs": [
      "Business perspectief",
      "Personas"
    ]
  },
  {
    "objectID": "business/personas/rse-personal-story.html",
    "href": "business/personas/rse-personal-story.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "(cl-infrastructure-rse-personal-story)= # Research Software Engineering Personal Story"
  },
  {
    "objectID": "business/personas/rse-personal-story.html#introduction",
    "href": "business/personas/rse-personal-story.html#introduction",
    "title": "Working document AI platform",
    "section": "Introduction",
    "text": "Introduction\nThe Society of Research Software Engineering was founded on the belief that a world which relies on software must recognise the people who develop it.\nAlthough originating in the UK, the concept of Research Software Engineering (RSE) has been widely adopted and specialised roles across academia, industry and other sectors are being recognised across the globe (see list of International RSE communities).\nIn this chapter, we share a personal story in the form of an open interview with Saranjeet Kaur Bhogal who has been spearheading efforts to establish recognition for RSE skills and roles in Asia.\nRecently, she has set up the RSE Asia Association to connect with different Asian researchers working in this space and establish pathways for early career researchers to build their skills in RSE in preparation for their RSE careers.\nBefore you dive into the interview, please note that we have a chapter describing the {ref}Research Software Engineering in Research Infrastructure Roles&lt;cl-infrastructure-rse&gt; as well as specific recommendation for {ref}starting a new community&lt;cl-new-community&gt;.\n\n\n\n```vopiuwzh ../../figures/rse-community.*\n\n\n\n\nheight: 400px\n\n\nname: rse-community\n\n\nalt: International RSE community is shown as a world map with different people helping each other jump from one continent to other. Especially shows people in India joining in the RSE network in Europe.\n\n\n\nRSE Asia Community joins the International RSE Network. This image was created by Scriberia for The Turing Way community and is used under a CC-BY 4.0 licence for reuse. Zenodo. DOI 10.5281/zenodo.3332807). ```\nTell us about your background, and what led you to set up the RSE Asia Association?\nMy name is Saranjeet Kaur Bhogal and I’m a Statistician based in India. I am interested to learn about open source and open science and like to remain involved with open science community work. In early 2021, I wrote the first draft of the R Development Guide through a project funded by the R Foundation. Furthermore, I co-lead the work on the outreach of the R Development Guide at the Digital Infrastructure Incubator at Code for Science & Society. Recently, I am working as a Technical Writer with The R Project at the Google Season of Docs 2022 for a project to “Expand and Reorganize the R Development Guide”. Previously, I have also worked with the Julia Language organization for Google Summer of Code 2020. In early 2022, I was selected as the founding committee of NumFOCUS Project Incubator. This Incubator is designed to support the growth of open-source scientific projects and communities and provide such projects with the required tools for becoming NumFOCUS Affiliated. I co-founded the Research Software Engineering (RSE) Asia Association during my participation in the Cohort 4 of the Open Life Science programme, to promote the RSE community and profession in the Asia region. I am also participating in the Pilot Mentorship Programme of the Society of Research Software Engineering to further build the RSE Asia community.\nWhat is the main motivation behind starting the RSE Asia Association?\nThe RSE Asia Association was started because there was a lack of representation from the Asian region in the international RSE community. Another reason was the significant timezone difference with the global north. Despite the shift of events to online mode due to the pandemic, the timezone difference remains a challenge for us when attending the events. Hence, we started RSE Asia to conduct events in a more Asia friendly timezone, and promote the RSE community and profession in Asia.\nHow did you get started?\nAlong with Jyoti Bhogal, I started by getting a structured mentorship for building RSE Asia. The Open Life Science programme’s Cohort-4 (OLS-4) is where we started scoping out our work and built initial social presence as well as public infrastructure like our website and Twitter account. We launched RSE Asia on 14th October 2021 during our participation in OLS-4. The launch invitation and our graduation video from OLS-4 are available on our website.\nWhat impact do you think is the RSE Asia Association is making in your community?\nWithin less than a year from our launch, we already have followers from around the work on our official Twitter account. I now represent RSE Asia as an observer at the international council of RSE. I was also invited to various RSE events taking place at the international levels, including direct mentorship and support from the Society of RSE. Based on these interactions and connections, it has become evident that representation from Asian communities had been long missing in the international RSE networks and RSE Asia Association is committed to address that. Thus, the impact I see is that RSE Asia is gaining momentum & receiving recognition in the international RSE community.\nWhat tools/software do you use most for the RSE Asia Association digital infrastructure?\nMostly GitHub organization, our organization website is hosted on GitHub.\nDo you have any top tips for other people that might be interested in starting a Research Software Engineering association/community?\nIf possible try to start with a structured mentoring programme, like the Open Life Science Programme - they can be immensely helpful in your community-building journey as they empower you with the best of tools and advice from the very start.\nDo you have any tips on things to avoid?\nTry to collaborate more and avoid working solo when you start with community building, it can get too big, too soon - so it is always helpful to have a group of collaborators along.\nAre there any other organisations/networks that you’re collaborating with?\nI (with my Co-Founder Jyoti) started building the RSE Asia Association as a project during my participation in the Open Life Science Cohort-4 (OLS-4) programme with Anne Fouilloux as our mentor. After OLS-4, I participated in the Pilot Mentoring Programme by the Society of Research Software Engineering to further build the community around RSE Asia under the mentorship of Michelle Barker, whereas Jyoti joined OLS-5 with a project to create onboardings pathways under the mentorship of Malvika Sharan. Recently, I presented talks about the work being done at RSE Asia at various events organized by groups and networks like the Asia Pacific Advanced Network (APAN53), the Research Software Alliance, the Collaborations Workshop (CW22) by the Software Sustainability Institute (All these talks are shared on our website.). We are open to collaborating with other organisations and networks too!\nApart from timezone differences what would you describe as the main challenge for your community?\nIt is a multinational association - so that is a unique challenge in itself. Several languages and cultures thrive in Asia and we are planning to set up a working group that includes representatives from different parts and nationalities of Asia - so that we have more understanding of the local research culture.\nWhat problems did you encounter along the way and what were some of the ways to solve them?\nLack of awareness about the term ‘Research Software Engineering’ in Asia has been challenging. To solve this we are planning to conduct some awareness events (more on that will be available on our website soon).\nWhat types of activities does RSE Asia support? Are there other activities you’d like to support in the future?\nWe participated in Hacktoberfest 2021. We are very keen on participating in online events/workshops/hackathons that help provide exposure to technical skills to our community members and also highlight the work that they do! In the future, we hope to co-organize some events with the Open Science communities and the international RSE community.\nWhich other organisations have influenced you the most when starting up RSE Asia?\nThe Society of Research Software Engineering has been the most influential one. I got introduced to the work of the Society during my participation in the useR! 2021 conference.\nHave you planned/do you foresee any governance structure for RSE Asia Association?\nWe are planning to set up a working group for RSE Asia soon. Setting up an Advisory Board is the next step toward bringing more structure to the organisation’s governance process.\nHave you been able to connect with other RSE associations across the globe? Has this been useful? How can other RSE associations help you get started?\nYes, we are planning to collaborate with RSE AUNZ for an event happening this year. I feel it is very useful to connect with the other RSE associations as there is always an exchange of ideas. Besides that, I am an observer at the international council of RSEs and represent RSE Asia there. At the council meetings, I get to interact with the representatives from various RSE associations and know more about their work. We are always open to discussing collaborations with the other RSE associations.\nAbout awareness raising – there is a similar issue in Latin America; to the best of my knowledge, the discussion around the need for RSEs hasn’t even started there yet! Is starting the RSE community a one-person effort? Or is there a critical mass of people pushing this effort?\nTo start with we are only a couple of people leading this initiative in Asia. Although we have immensely benefited from various Mentorship programmes and Mentors, there is still a need to get more people involved from Asia to push the effort forward. With the hope to achieve this, we are planning to set up a working group soon.\nWhere do you see the RSE Asia Association going in the future?\nI feel the RSE Asia Association would expand more and include more representatives across the Asian region in the future. We are setting up a Working Group and an Advisory Board for RSE Asia, details for which are shared on the RSE Asia Association.\nThis personal story in the form of an open interview was originally written by Saranjeet Kaur Bhogal during the Turing Way book Dash in May 2022. We acknowledge all the people who contributed their time asking questions and reviewing the written draft: Anne Lee Steele, Arielle Bennett, Carlos Martinez, Elisa Rodenburg, Emma Karoune, Esther Plomp, Kim Martin, Lena Karvovskaya and Malvika Sharan (names are in alphabetical order)."
  },
  {
    "objectID": "business/personas/rse.html",
    "href": "business/personas/rse.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "(cl-infrastructure-rse)= # Research Software Engineer: Overview\nResearch Software Engineers (RSEs) are programmers with scientific backgrounds who play increasingly critical roles in the conduct of research and production of research software tools. They are highly skilled team members who can also conduct their own research as part of their role.\n(cl-infrastructure-rse-role)= ## What do Research Software Engineers do? RSEs work on research projects. They may be assigned to projects based on skills or background from a pool of other RSEs, or be specifically hired to work on a project.\nAdditionally, RSEs can also be PIs and run their own independent research projects, depending on the institution and policies of the projects they are working on."
  },
  {
    "objectID": "business/personas/rse.html#who-are-research-software-engineers",
    "href": "business/personas/rse.html#who-are-research-software-engineers",
    "title": "Working document AI platform",
    "section": "Who are Research Software Engineers?",
    "text": "Who are Research Software Engineers?\nRSEs typically have a scientific or research background, often developing programming skills during masters or doctoral studies. You can find out more about the backgrounds of RSEs through the Software Sustinability Institute’s 2018 Survey."
  },
  {
    "objectID": "business/personas/rse.html#what-qualifications-or-skills-do-you-need-to-be-a-research-software-engineer",
    "href": "business/personas/rse.html#what-qualifications-or-skills-do-you-need-to-be-a-research-software-engineer",
    "title": "Working document AI platform",
    "section": "What qualifications or skills do you need to be a Research Software Engineer?",
    "text": "What qualifications or skills do you need to be a Research Software Engineer?\nThere are not specific formal qualifications needed to become an RSE, but the majority will have at least one advanced degree (masters or doctoral level). However, all RSEs are able to program, with the majority coding in Python, SQL, R, C/C++ or JavaScript. RSEs are also likely to understand concepts such as agile development, integration and testing, software architecture, and version control."
  },
  {
    "objectID": "business/personas/rse.html#challenges-for-research-software-engineers",
    "href": "business/personas/rse.html#challenges-for-research-software-engineers",
    "title": "Working document AI platform",
    "section": "Challenges for Research Software Engineers",
    "text": "Challenges for Research Software Engineers\n\nLack of formal pathways for development\nProduction of software and tools not always recognised as a research output\nRSEs not viewed as researchers in their own right\nAdjusting to working on different projects, possibly far from their original background"
  },
  {
    "objectID": "business/personas/rse.html#benefits-of-having-research-software-engineers",
    "href": "business/personas/rse.html#benefits-of-having-research-software-engineers",
    "title": "Working document AI platform",
    "section": "Benefits of having Research Software Engineers",
    "text": "Benefits of having Research Software Engineers\n\nHighly technical skills that support researchers who cannot program\nSharing of best practices in research software engineering across projects\nApply cross-disciplinary knowledge to different projects\nSoftware will be more reliable and robust, supporting reuse and reproducibility\n\n(cl-infrastructure-rse-support)= ## Organisations that support Research Software Engineers * Society for Research Engineering * Software Sustainability Institute\n(cl-infrastructure-rse-summary)= ## Summary Research Software Engineers are highly skilled, valuable members of any research group that is conducting computational research. They bring technical programming skills as well as best practices from software architecture and open source development to academic research. Some also conduct their own independent research projects."
  },
  {
    "objectID": "business/personas/ram.html",
    "href": "business/personas/ram.html",
    "title": "Working document AI platform",
    "section": "",
    "text": "(cl-infrastructure-ram)= # Research Application Managers: Overview"
  },
  {
    "objectID": "business/personas/ram.html#context-and-the-unmet-need",
    "href": "business/personas/ram.html#context-and-the-unmet-need",
    "title": "Working document AI platform",
    "section": "Context and the unmet need",
    "text": "Context and the unmet need\nAcademic incentives encourage the creation of novel knowledge, such as creating new machine learning algorithms. Often, after ground-breaking work has been published in an academic journal, the algorithm and the software built to deploy it are not supported because the researcher begins a new project or moves to another institution. Traditionally, academia is less interested in supporting and rewarding work on: - Improving and extending existing research outputs/software - Promoting interoperability of new and existing outputs/software - Investing in usability, re-usability and user-friendliness of outputs/software (new and existing) - Co-creating outputs with users from the early stages of the research output lifecycle - Proactively discovering new real-world applications and use cases beyond the original academic field and investing in their promotion, adaptation and adoption\nIn other words, academia highly prizes knowledge creation. Investment in dissemination, interconnectivity and usability beyond the original academic field is typically not rewarded with the greatest academic prizes. In the highly competitive world of early career research, such work is therefore often not prioritised.\n\n\n\n```pnfbhxdy ../../figures/research-application-managers.*\n\n\n\n\nname: research-application-managers\n\n\nalt: An illustration depicting the animal rams as the research application managers who are connecting with users, applying research in real world and facilitating innovative process in research infrastructure.\n\n\n\nResearch Application Managers work with the research team to embed outputs into user organisations. The Turing Way Community, & Scriberia. (2020, November). Illustrations from the Turing Way book dashes. Zenodo. http://doi.org/10.5281/zenodo.4323154 ```"
  },
  {
    "objectID": "business/personas/ram.html#a-ram-as-a-connecting-role-in-the-research-ecosystem",
    "href": "business/personas/ram.html#a-ram-as-a-connecting-role-in-the-research-ecosystem",
    "title": "Working document AI platform",
    "section": "A RAM as a connecting role in the research ecosystem",
    "text": "A RAM as a connecting role in the research ecosystem\nWe have created the role of a Research Application Manager (RAM) at The Alan Turing Institute with the aim of addressing this incentive gap. RAMs are specifically incentivised and rewarded to conduct the work that traditionally sits on the margins for a typical early career researcher engaging in creating outputs such as open source software.\nRAMs are in part inspired by the role of a Product Manager in tech firms. They also have similarities with the role of a Developer Advocate and their work overlaps with that of the Community Manager. In some cases the RAM workload may include detailed, often technical, input, resembling a Solutions Engineer or a Forward Deployed Engineer, who interface directly with customers to ensure that the tool meets user needs.\nThe measures of success for a RAM: - Engaging with the research team early on in the project to bring the perspective of potential users of their software tools and to proactively co-create from the early stages - Advocating for the user perspective throughout the development and feature prioritisation process - Identifying the key users and the target audience - Researching and understanding the user community - Engaging with the user community - Identifying sustainability and funding solutions in collaboration with the research team - Promoting the tools outside the academic field of the original research team - Approaching the output as a research “product” and bringing an appropriate level of “market intelligence” to the academic team - “Packaging” or “re-packaging” the tool to improve usability and accessibility to different audiences\nRAMs are a solution primarily to the incentive gap problem. They bring a “Team Science” mindset to the research teams and promote research best practices - a task they share with the Community Manager role.\nBoth RAMs and CMs promote best practices in: - Interoperability of outputs - Reproducibility of research findings and outputs - Team science and open science - Ethics - Co-creation - Inclusivity\nCommunity managers are becoming an established role in the research ecosystem and in open source software communities in particular. We hope that RAMs will have a similarly successful trajectory.\nKey RAM values: adaptability, collaboration and legacy"
  },
  {
    "objectID": "drafts/kapseli.html",
    "href": "drafts/kapseli.html",
    "title": "Kapseli",
    "section": "",
    "text": "uitleg Kapseli\nzie ook details in EOSC-Entrust blueprint"
  },
  {
    "objectID": "drafts/andrea.html",
    "href": "drafts/andrea.html",
    "title": "AnDREa",
    "section": "",
    "text": "https://andrea-cloud.com/, currently run by [add list of nodes that run AnDREa]\n\nTrusted Processing Environment\nProvides generic services such as access control, workspace configuration etc. as the main service offering\nUsers are free to install and run whatever software they want on a virtual machine\nFederated learning currently under development"
  },
  {
    "objectID": "drafts/plugin.html",
    "href": "drafts/plugin.html",
    "title": "PLUGIN en vantage6",
    "section": "",
    "text": "Met PLUGIN willen we één federatief platform neerzetten van en voor de Nederlandse ziekenhuizen. Dit elimineert de behoefte aan maatwerk infrastructuur, wat resulteert in een efficiëntere gegevensuitwisseling en lagere kosten voor het ziekenhuis.\n\n\n\nVia PLUGIN is het mogelijk om AI-modellen te trainen met data van verschillende ziekenhuizen, wat de schaalbaarheid naar andere ziekenhuizen verhoogt. De betrokken ziekenhuizen hebben echter geen toegang tot de inviduele gegevens van andere ziekenhuizen.\n\n\n\nHet omzetten van klinische gegevens naar het FHIR-formaat vergemakkelijkt de analyse en uitwisseling van deze informatie. Met FHIR-standaardisatie aan de bron biedt PLUGIN één infrastructuur die voor meerdere doeleinden gebruikt kan worden."
  },
  {
    "objectID": "drafts/plugin.html#waarom-plugin",
    "href": "drafts/plugin.html#waarom-plugin",
    "title": "PLUGIN en vantage6",
    "section": "",
    "text": "Met PLUGIN willen we één federatief platform neerzetten van en voor de Nederlandse ziekenhuizen. Dit elimineert de behoefte aan maatwerk infrastructuur, wat resulteert in een efficiëntere gegevensuitwisseling en lagere kosten voor het ziekenhuis.\n\n\n\nVia PLUGIN is het mogelijk om AI-modellen te trainen met data van verschillende ziekenhuizen, wat de schaalbaarheid naar andere ziekenhuizen verhoogt. De betrokken ziekenhuizen hebben echter geen toegang tot de inviduele gegevens van andere ziekenhuizen.\n\n\n\nHet omzetten van klinische gegevens naar het FHIR-formaat vergemakkelijkt de analyse en uitwisseling van deze informatie. Met FHIR-standaardisatie aan de bron biedt PLUGIN één infrastructuur die voor meerdere doeleinden gebruikt kan worden."
  },
  {
    "objectID": "drafts/plugin.html#vantage6",
    "href": "drafts/plugin.html#vantage6",
    "title": "PLUGIN en vantage6",
    "section": "vantage6",
    "text": "vantage6\n\nBeveiliging via gebruikers op centrale server\nGoedkeuring via container registry: welke berekeningen mogen worden uitgevoerd"
  }
]