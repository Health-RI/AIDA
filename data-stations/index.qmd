---
title: "Layers, pipes and patterns: detailing the concept of data stations as a foundational building block for federated data systems in healthcare"
lang: en
# author:
#   - name: Daniel Kapitan
#     affiliations: 
#       - name: Health-RI
#         city: Utrecht
#         country: the Netherlands
#       - name: Dutch Hospital Data
#         city: Utrecht
#         country: the Netherlands
#       - name: Eindhoven University of Technology
#         city: Eindhoven
#         country: the Netherlands
#     orcid: 0000-0001-8979-9194
#     email: daniel@kapitan.net
#     url: https://kapitan.net
#     attributes:
#       corresponding: true
#   - name: Jack Broeren
#     affiliations:
#       - name: Health-RI
#         city: Utrecht
#         country: the Netherlands
#   - name: Jeroen Beliën
#     affiliations:
#       - name: Health-RI
#         city: Utrecht
#         country: the Netherlands
#       - name: Amsterdam UMC
#         city: Amsterdam
#         country: the Netherlands
#   - name: Niels Bolding
#     affiliations:
#       - name: Health-RI
#         city: Utrecht
#         country: the Netherlands
#   - name: Stefan van der Loop
#     affiliations:
#       - name: Cumuluz
#         city: Utrecht
#         country: the Netherlands
#   - name: Yannick Vinkesteijn
#     affiliations:
#       - name: Dutch Hospital Data
#         city: Utrecht
#         country: the Netherlands
#     orcid: 0009-0009-7761-5381
#     email: y.vinkesteijn@dhd.nl
#     url: https://yannickvinkesteijn.com/
#   - name: Joep de Ligt
#     affiliations: 
#       - name: Hartwig Medical Foundation
#         city: Amterdam
#         country: the Netherlands
#     orcid: 0000-0002-0348-419X
#     email: j.deligt@hartwigmedicalfoundation.nl
#     url: https://hartwigmedicalfoundation.nl
abstract: |
  We describe ...
keywords: [personal health train, federated learning, data systems, data engineering, lakehouse, Apache Arrow, Apache Parquet, Apache Icebert, duckdb, polars]
pdf: index.pdf
index-terms: 
  - Scientific writing
  - Typesetting
  - Document creation
  - Syntax
citation: 
  container-title: GitHub
  page: 1-3
  type: article-journal  
  issued: 2025-06-01
  url: https://github.com/health-ri/aida
  pdf-url: index.pdf
fig-cap-location: top
---

## A shift towards federated data systems as a design paradigm

The ambition for a seamlessly connected digital healthcare ecosystem, capable of leveraging vast quantities of patient data remains illusive. Designing and implementing health data platforms is notoriously difficult, given the heterogeneity and complexity of such systems. To address these issues, federated data systems have emerged as a design paradigm. This approach enables data to remain securely at its source, while allowing for distributed analysis and the generation of collective knowledge.

Recent technological inventions offer important new enablers to implement federated data systems, most notably:

- Capabilities of edge computing and single-node computing has increased significantly whereby it is now possible to process up to 1 TB of tabular data on a single node thereby enabling large volumes of data processing to be done efficiently in a decentralized fashion [@raasveldt2019duckdb;@nahrstedt2024empirical];
- Federated analytics [@wang2025survey], and specifically federated learning, has matured as a means for training of predictive models, most notable through weights sharing of deep learning networks [@rieke2020future;@teo2024federated];
- Privacy-enhancing technologies (PETs) such as secure multi-party computation (MPC) significantly improve secure processing across a network of participants and are now sufficiently mature to be used on an industrial scale [@un2023pet-guide;@royalsociety2023privacy];
- The composable data stack provides a way to unbundle the venerable relational database into loosely components, thereby making it easier and more practical to implement FDS using cloud-based components with microservices, thereby opening up a transition path towards more modular and robust architectures for FDS [@pedreira2023composable;@composable].

The architectural shift from centralized to federated data systems is not merely a technical evolution. Modern approaches to data governance are undergoing a similar paradigm shift towards federated solutions. As an example, the concept of a [data mesh](https://martinfowler.com/articles/data-mesh-principles.html) is increasingly being adopted at large corporations. From the perspective of sovereignty and solidarity, we believe that a commons-based, federated approach has distinct benefits in moving towards a more equitable, open digital infrastructure [@krewer2024digital]. Federated data systems are inherently more aligned with contemporary data governance frameworks, including the Data Governance Act (DGA), the European Health Data Space (EHDS) and the concept of data solidarity [@prainsack2023beyond].

However, this ongoing paradigm shift towards is not without challenges. The notion of what constitutes a federated data system needs to be defined in more detail if we are to see the forest for the trees between different instantiations of the same concept. For example, 'federation' can mean any of the following solution patterns:

- Data federation addresses the problem of uniformly accessing multiple, possibly heterogeneous data sources, by mapping them into a unified schema, such as an RDF(S)/OWL ontology or a relational schema, and by supporting the execution of queries, like SPARQL or SQL queries, over that unified schema [@gu2022systematic];
- Federation within the context of a Personal Health Train (PHT) refers to the concept by data processing is brought to the (personal health) data rather than the other way around, allowing (private) data accessed to be controlled, and to observe ethical and legal concerns [@beyan2020distributed;@choudhury2020personal;@boninodasilvasantos2022personal;@zhang2023secure], and is just one of many solutions designs that are collectively grouped as federated analytics [@wang2025survey];
- Federation as a mechanism for data sharing in a temporary staging environment within a network of research organizations in a Trusted Research Environment (TRE), with different types of federations services (localization, access);
- Federation services as defined in the DSSC Blueprint 2.0 pertain to the support the interplay of participants in a data space, operating in accordance to the policies and rules specified in the Rulebook by the data space authority.

What then, is a viable development path out of this creative chaos?

## Data stations as a foundational building block

Inspired by previous calls to action to move towards open architectures for health data systems [@estrin2010health;@mehl2023fullstac] and the notion of the hourglass model [@estrin2010health;@beck2019hourglass;@schultes2023fair], we hypothesize that the concept of a 'data station' can be used as a foundational building block for federated data systems. A data station should provide a set of minimal standards (at the waist of the hourglass), thereby maximizing the freedom to operate between data providers and data consumers within the context of a health data space. Note that this approach has many similarities with the FAIR Hourglass [@boninodasilvasantos2022fair;@schultes2023fair]. Our approach of data stations presented here focuses on enabling on secondary data use of routine collected clinical data using the architecture of the Personal Health Train as a starting point [@beyan2020distributed;@choudhury2020personal;@boninodasilvasantos2022personal;@zhang2023secure]. The objective of this paper is to extend this architecture in order to address four design questions that are relevant in ongoing efforts to implement nation-wide federated systems.

### Online transactional vs. analytical processing 
First, it is well known data systems have different design an performance characteristics depending whether they are built for online transactional processing (OLTP) or online analytical processing (OLAP), as summarized in the @tbl-oltp-olap below (taken from @kleppmann2026designing).

|Property|Transaction processing systems (OLTP)|Analytic systems (OLAP)|
|---|---|---|
|Main read pattern|Small number of records per query, fetched by key|Aggregate over large number of records|
|Main write pattern|Random-access, low-latency writes from user input|Bulk import (ETL) or event stream|
|Data modeling|Predefined |Defined post-hoc, either schema-on-read or schema-on-write |
|Primarily used by|End user/customer, via web application|Internal analyst, for decision support|
|What data represents|Latest state of data (current point in time)|History of events that happened over time|
|Dataset size|Gigabytes to terabytes|Terabytes to petabytes|

: Distinguishing characteristics between online transactional and analytical systems {#tbl-oltp-olap}

Current efforts to design and implement the EHDS in fact aims to support primary (i.e. OLTP) and secondary use (OLAP) in one go [@ehds;@cascini2024health]. This Herculean endeavour has spawned many initiatives to develop a coherent architecture and support implementation across Europe that ultimately should lead to interoperability in the broadest sense of the word, most notably:

- The Data Space Blueprint v2.0 (DSB2) by the Data Spaces Support Centre ([@dssc2025data]) that serves as a vital guide for organizations building and participating in data spaces.
- The Simpl Programme ([@simpl]) that aims to develop an open source, secure middleware that supports data access and interoperability in European data initiatives. It provides multiple compatible components, free to use, that adhere to a common standard of data quality and data sharing.
- TEHDAS2 ([@tehdas2]), a joint action that prepares the ground for the harmonised implementation of the secondary use of health data in the EHDS.

We believe, however, that in order to successfully design and implement health data spaces, more detailed analysis and solution patterns are required that distinguish between primary (OLTP) and secondary (OLAP) data use. Although functional components can be shared between these two, it is a matter of the devil being in the details. Hence one of the objectives of this paper is to detail an open, technology agnostic architecture for secondary use, to complement existing efforts and guide the development in the field.

### Centralized vs decentralized processing
A second design question pertains to the choice of single-node (centralized) or distributed (decentralized) platforms, which are not only be driven by technical considerations (scalability, elasticity, fault tolerance, latency) but are also strongly dependent on organizational, legal or regulatory requirements such as data residency. The general approach of EHDS and other data spaces is federative by nature, that is, decentralized. For example, DSB2 stresses the need for interoperability and federative protocols within and across data spaces.

Upon closer inspection, however, specific functional components that are foreseen within the EHDS are best characterized as centralized (sub-)systems. As an example, consider the secure processing environments (SPE) as defined in [recital 77 of the EHDS](https://eur-lex.europa.eu/legal-content/en/TXT/HTML/?uri=OJ:L_202500327#rct_77). Known examples of such SPEs include data platforms provided by national statistics offices (CBS Microdata environment), healthcare-specific national platforms (Finland's Kapseli platform) and Trusted Research Environments (TREs) within the domain of research (see [EOSC-ENTRUST](https://eosc-entrust.eu/) for examples across Europe). Given that healthcare data is often vertically partitioned (data elements of the same subject are scattered across various data holders), SPEs provide the most effective means to (temporarily) share, integrate and analyse such data. Hence many SPEs are best described as centralized systems, and thus we need to take into account that data spaces constitute a hybrid architecture that includes both centralized and decentralized components. Thus a more detailed analysis is required to arrive at scalable solution patterns that combine centralized vs. decentralized processing in a larger federated health data system.

### Solution patterns to populate data stations
A third design issue pertains to the mechanisms through which the data stations are to be populated with data by the data holder. In essence the industry is converging towards solution patterns from data engineering and data warehousing. OpenHIE specification, for example, makes a distinction of a Shared Health Record as a OLTP system for primary health data use vis-a-vis FHIR data pipelines that populate data stations for analytical, secondary use. [TO DO: improve wording, add references]

### Federation as system of systems
The fourth design challenge pertains to the need for having a 'system of systems' [@gorod2008paradox] in the federated health data system at large. In real-world setting, secondary health data sharing will need to take into account the limited resources and expertise of smaller health care providers. In fact, the EHDS explicitly addresses this issue in article [article 50](https://eur-lex.europa.eu/legal-content/en/TXT/HTML/?uri=OJ:L_202500327#art_50) that exempts micro enterprises from the obligation to directly participate in the EHDS, but that each member state may opt to form so-called data intermediaries to act as a go-between ([recital 59](https://eur-lex.europa.eu/legal-content/en/TXT/HTML/?uri=OJ:L_202500327#rct_59)). Along a different dimension, we foresee a system of systems of various domain-specific federated networks that are loosely coupled, which poses design challenges in terms of autonomy (to what extend can each sub-network make independent choices), connectivity (how to connect sub-systems) and diversity.

### Outline

In this paper we take loosely follow an Action Design Research approach [@sein2011action;@venable2016feds] to address these design questions. Our main contributions are:

- A harmonized ontology of a data station and data hub, that integrates the PHT architecture [@boninodasilvasantos2022personal] and the DSSC Blueprint 2.0
- Comparative analysis of existing implementations
- Synthesis of the above into functional and technical description of a data station in Archimate, thereby focusing on two primary patterns [@buschmann1996system]:
  - the layers pattern for addressing various aspects of interoperability across the stack, extending earlier work by @welten2024study who have introduced five layers of interoperability; 
  - the pipes and filters pattern for addressing various solution designs for the extract-transform-load (ETL) mechanisms through which data stations are populated, following the current common practice of datalakes and lakehouse solution designs [@harby2025data;@schneider2024lakehouse;@hai2023data;@abouzaid2025building;@mazumdar2023data];
- A reference implementation of data stations using the trains based on containers as a generic infrastructure for federated learning and federated analysis.

## Towards a unified ontology of data stations and data hubs

### The Personal Health Train architecture

We take the Personal Health Train (PHT) architecture as our starting point [@boninodasilvasantos2022personal], wherein a Data Station is defined as a software application primarily responsible for making data and its associated metadata available to users under conditions determined by applicable regulations and Data Controllers. The main concepts are shown in @fig-pht-roles-components, while the various types of trains are shown in @fig-pht-train-types. A glossary of the concept is provided in @sec-glossary.

::: {#fig-pht layout-nrow=2}

![Main roles and components](./images/pht-roles-components.png){#fig-pht-roles-components}

![Train types](./images/pht-train-types.png){#fig-pht-train-types}

High-level overview of the Personal Health Train (PHT) architecture
:::

In their paper @boninodasilvasantos2022personal continu to describe more details of the PHT architecture including i) the various functions, services, interface and internal components of the data station; ii) the data visiting process; and iii) the data staging concept in the case data access has been authorized, but the station is not capable of executing the train and needs to stage a capable station with enough resources to run the train. We will consider these details later.

As an aside, it is good to mention that the authors of the PHT architecture have initiated the development of two specifications after publication of this paper, namely:

- [FAIR Data Point](https://specs.fairdatapoint.org/fdp-specs-v1.2.html) specification, which covers only the metadata and catalog part of the PHT architecture; 
- the [FAIR Data Train](https://specs.fairdatatrain.org/) specification, which covers the full scope of the original paper but at the time of writing is still incomplete.

### Mapping PHT to the DSSC Blueprint 2.0

To arrive at consistent conceptualization of data stations and trains, @tbl-mapping-pht-dssc maps the PHT architecture to the DSSC Blueprint 2.0 (DSB2). Some mappings are relatively evident. For example, the concept of Data and Metadata as defined in PHT is subsumed in the concept of a Data Product in DSB2. Less evident, is the mapping of the notion of a Train that '... represents the way data consumers interact with the data available in the Data Stations. Trains represent a particular data access request and, therefore, each train carries information about who is responsible for the request, the required data, what will be done with the data, what it expects from the station, etc.' to Value Creation Services in DSB2 that includes data fusion and enrichment, collaborative data analytics and federated learning. We tentatively conclude that it is possible to have a consistent conceptual mapping between, at least at the high level, of the PHT architecture into DSB2. We will return to this matter, when more detailed functions and technical standards are considered for the Archimate specification.

+-----------------------+-------------------------------------------------------------------------------------------------------------+
| Component PHT         | mapping to DSSC Blueprint 2.0 concepts                                                                      |
+:======================+:============================================================================================================+
| - Data Station        | - Data Space Building Block                                                                                 |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Data                | - [Data Product](https://dssc.eu/space/BVE2/1071253231/Data+Space+Offering#3.1.-Data-products)              |
| - Metadata            |                                                                                                             |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Data Controller     | - Data Rights Holder                                                                                        |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Station Controller  | - Data Product Provider                                                                                     |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Personal Gateway    | - included in Participant Agent Services                                                                    |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Station Directory   | - included in Federation Services                                                                           |
|                       | - Catalogue provisions and discovers offerings of data and services in a data space                         |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Directory Owner     | - Common Intermediary provides federation services that are common to all participants of the data space    |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Train               | - Value Creation Services                                                                                   |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Train Provider      | - Service Provider                                                                                          |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Train Handler       | - specialization of Data Space Component that realizes the Train Value Creation Service                     |
+-----------------------+-------------------------------------------------------------------------------------------------------------+
| - Train Owner         | - included in Service Provider as most generic role                                                         |
|                       | - concept of Intermediary (specialization of SP) is closer to definition of Train Owner                     |
+-----------------------+-------------------------------------------------------------------------------------------------------------+

: Mapping the key concepts from the PHT architecture [@boninodasilvasantos2022personal] to the [concepts of the DSSC Blueprint 2.0](https://dssc.eu/space/BVE2/1071252161/Alphabetical+List+of+All+Defined+Terms+in+Blueprint+v2.0). {#tbl-mapping-pht-dssc}

### The lakehouse architecture as the _de facto_ standard for populating data stations

The PHT architecture does not specify _how_ the data stations should be populated with data. Also the DSB2 only describes how the 'Data, Services and Offerings descriptions' building block should provide data providers the tools to describe a data product appropriately and completely, that is, tools for metadata creation and management.

 
One of the key questions of this paper is to detail [the 'data conformity zone' as defined in the Cumuluz canvas](https://www.vzvz.nl/media/downloads/canvas-voor-databeschikbaarheid-v11-003pdf/download) as the functionality through which the data station is populated


## Comparative analyses of existing data stations



## Synthesis of solution patterns patterns for data stations 

We take Klepmann (2017) as our starting point, who states that "Many applications today are _data-intensive_, as opposed to _compute-intensive_. Raw CPU power is rarely a limiting factor for these applications—bigger problems are usually the amount of data, the complexity of data, and the speed at which it is changing."

Generically, we want:

| Reliability                           | Scalability                     | Maintainability                        |
| ------------------------------------- | ------------------------------- | -------------------------------------- |
| tolerating hardware & software vaults | Measuring load & performance    | Operability, simplicity & evolvability |
| human error                           | Latency percentiles, throughput |                                        |

We focus on analytical data systems, with different patterns from transactional data systems.



### Detailing the layers of a data station

TO DO: provide detailed layers, and explain how interoperability works across the layers:

- Storage layer (technology): all reference architectures stipulate use of S3-compliant blob storage
- Data and metadata (application): resides in the data station
- We propose to move towards open table formats, that is, Apache Iceberg, whereby storage and compute can be separated

![Solution of a minimal lakehouse that sits at the core of a data station](./images/minimal-lakehouse.png){#fig-minimal-lakehouse}


### Detailing the data conformity zone

TO DO: explain that

- data conformity zone is essentialy a lakehouse pattern
- the architecture of a lakehouse has stabilized and converged towards:

  - **Colum-oriented storage and memory layout:** Apache Arrow ecosystem, including Apache Flight
  - **Late-binding with logical data models most suited for analytics:** ELT pattern with zonal architecture
    - *staging zone:* hard business rules (does incoming data comply to syntactic standard), change data capture 
    - *linkage & conformity zone:* concept-oriented tables, typically following a data vault modeling principle, ascertain referential integrity across resources, with tables per concept and linking tables. Mapping to coding systems. Entity resolution for record linkage at the subject level
    - *consumption zone:* convenient standardized views like an event table (patient journey, layout for process mining) with uniformity of dimensions using a star schema

### Detailing the trains

TO DO: explain

- difference between centralized and distributed federated learning (causes lots of confusion)
- basically Train is a generalization of all types of computes
- difference between
  - Train for secondary use, which usually with batch-wise, less strict latency requirements
  - Train for primary use, like API call and messaging, with stricter latency requirements. This also includes deployment of AI for inference




## Reference implementation with current open source software (OSS) components

### Data stations from the composable data stack

- Python and SQL(-like) languages as the *de facto* standard for analytical processing i.e. the most commonly used analytical scripting languages. Where necessary, using Intermediate Representations (IR), any analytical query can be transpiled to the target engine of choice
- Single-node compute capable of efficiently processing up to 1 TB of data within tens of seconds (polars, DuckDB), so we do away with distributed processing
- Open table formats (Iceberg, Hudi, Delta) and open file formats (parquet, AVRO)

### Container-based trains as the most versatile solution

### Bringing it all together for federated analytics & machine learning (FL)

- Local data stations are conceptualized as serverless lakehouses
	- Local ELT pipelines
	- Decentralized (pre-)processing, including quality control upon ingest
	- ...
- For horizontally partioned data, we can apply FL techniques where only aggregated results are combined centrally
- For vertically partitioned data, we need an intermediate/temporary zone for linking the data
- For both horizontally and vertically partitioned data, we can choose to add PETs, most specifically MPC, as an extra security measure
	- Horizontally partitioned data: [one-shot FL](https://rosemanlabs.com/en/blogs/privacy-safe-federated-learning)
	- Vertically partitioned data:
  	- linkage in the blind
  	- reversible pseudonimization
- standardized approach to mapppings [@zhang2024crossstandard]


## Parking lot

- Difference with data mesh: mesh of domains, federation is in the same domain. Underlying technology of a data station, however, is functionally identical
- UMCU: CQRS pattern for separately optimizing read/write patterns
- DSSC Blueprint: FL subsumed in value adding services

@tbl-types-hdp lists known examples of existing health data platform architectures along these two trade-offs.

|               | primary           | secondary       |
|---------------|-------------------|-----------------|
| **centralized**   | openHIE [@2024openhie], Digizorg, Nordics | kapseli, Mayo, ... |
| **decentralized** | RSO Zuid Limburg, Twiin portaal, ...  | many federated analytics research networks such as x-omics programme and EUCAIM |

: Broad categorization of health data platforms {#tbl-types-hdp}


## Glossary {#sec-glossary}

- *Data Controller* (Business Role) is the role of controlling rights over data.
- *Data Station* (Business Product) is a software application responsible for making data and their related metadata available to users under the accessibility conditions determined by applicable regulations and the related Data Controllers.
- *Directory Owner* (Business Role) is the role of being responsible for the operation of a Station Directory.
- *Personal Gateway* (Business Product) is a software application responsible for mediating the communication between Data Stations and Data Controllers. The Data Controllers are able to exercise their control over the data available in different Data Stations through the Personal Gateway.
- *Station Directory* (Business Product) is a software application responsible for indexing metadata from the reachable Data Stations, allowing users to search for data available in those stations.
- *Train* (Business Representation) represents the way data consumers interact with the data available in the Data Stations. Trains represent a particular data access request and, therefore, each train carries information about who is responsible for the request, the required data, what will be done with the data, what it expects from the station, etc.
- *Train Handler* (Business Representation) is a software application that interacts with the Stations Directory on behalf of a client to discover the availability and location of data and sends Trains to Data Stations.
- *Station Owner* (Business Role) is the role of being responsible for the operation of a Data Station.
- *Train Owner* (Business Role) is the role of using a Train Handler to send Trains to Data Stations.
- *Train Provider* (Business Role) is the role of being responsible for the creation of a specific Train, e.g. the developer of a specific analysis algorithm.

